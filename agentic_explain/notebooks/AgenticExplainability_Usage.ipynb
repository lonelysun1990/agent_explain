{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Agentic Explainability Workflow – Usage\n",
        "\n",
        "This notebook runs the agentic explainability pipeline: ask a natural-language question about optimization results, get a counterfactual run and a summary (trade-offs or infeasibility conflict).\n",
        "\n",
        "**Prerequisites:**\n",
        "- `config/secrets.env` with `OPENAI_API_KEY` and `GUROBI_LICENSE_FILE=config/WLS-dev-key.lic`\n",
        "- Run the first two sections once to create the baseline and RAG index; then you can run only the workflow section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup paths and load secrets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PID: 38325\n",
            "{'Kernel ID': ['/Users/Larry.Jin/miniconda3/envs/rag/lib/python3.11/site-packages/ipykernel_launcher.py',\n",
            "               '--f=/Users/Larry.Jin/Library/Jupyter/runtime/kernel-v3492970f19098298ea80574fb0c222b563e691e6e.json']}\n"
          ]
        }
      ],
      "source": [
        "# check Jupyter kernel ID\n",
        "import sys, os\n",
        "print(\"PID:\", os.getpid())\n",
        "import pprint\n",
        "pprint.pprint({\"Kernel ID\": sys.argv})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: /Users/Larry.Jin/Documents/research/agent_explain\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Project root: directory that contains agentic_explain and use_case\n",
        "def find_project_root():\n",
        "    for start in [Path.cwd()] + list(Path.cwd().parents):\n",
        "        if (start / \"agentic_explain\").is_dir() and (start / \"use_case\").is_dir():\n",
        "            return start\n",
        "    return Path.cwd()\n",
        "\n",
        "PROJECT_ROOT = find_project_root()\n",
        "\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(PROJECT_ROOT))\n",
        "\n",
        "from config.load_secrets import load_secrets, get_gurobi_env_kwargs\n",
        "\n",
        "load_secrets()  # load from config/secrets.env or project root\n",
        "print(\"Project root:\", PROJECT_ROOT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Run baseline (once)\n",
        "\n",
        "Saves `outputs/baseline_result.json`, `outputs/model.lp`, `outputs/model.mps`. Skip this cell if you already have them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping Gurobi model run: baseline outputs already exist in /Users/Larry.Jin/Documents/research/agent_explain/use_case/staffing_model/outputs\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from gurobipy import GRB\n",
        "\n",
        "from use_case.staffing_model import load_raw_data, process_data, build_gurobi_model\n",
        "\n",
        "from use_case.staffing_model import STAFFING_DATA_DIR, STAFFING_OUTPUTS_DIR\n",
        "DATA_DIR = STAFFING_DATA_DIR\n",
        "OUTPUTS_DIR = STAFFING_OUTPUTS_DIR\n",
        "OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Controller: if baseline outputs already exist, skip running Gurobi\n",
        "SKIP_IF_BASELINE_EXISTS = True  # Set this to False to force rerun\n",
        "\n",
        "baseline_result_path = OUTPUTS_DIR / \"baseline_result.json\"\n",
        "model_lp_path = OUTPUTS_DIR / \"model.lp\"\n",
        "model_mps_path = OUTPUTS_DIR / \"model.mps\"\n",
        "\n",
        "raw = load_raw_data(DATA_DIR)\n",
        "inputs = process_data(\n",
        "    raw[\"fte_mapping\"],\n",
        "    raw[\"concurrent_projects\"],\n",
        "    raw[\"oversight_ds_list\"],\n",
        "    raw[\"ds_list\"],\n",
        "    raw[\"project_list\"],\n",
        ")\n",
        "env_kwargs = get_gurobi_env_kwargs()\n",
        "\n",
        "def baseline_outputs_exist():\n",
        "    return baseline_result_path.exists() and model_lp_path.exists() and model_mps_path.exists()\n",
        "\n",
        "if SKIP_IF_BASELINE_EXISTS and baseline_outputs_exist():\n",
        "    print(\"Skipping Gurobi model run: baseline outputs already exist in\", OUTPUTS_DIR)\n",
        "else:\n",
        "    model = build_gurobi_model(inputs, env_kwargs)\n",
        "    model.setParam(GRB.Param.TimeLimit, 100)\n",
        "    model.optimize()\n",
        "\n",
        "    if model.status in (GRB.OPTIMAL, GRB.TIME_LIMIT):\n",
        "        baseline_result = {\n",
        "            \"status\": \"optimal\" if model.status == GRB.OPTIMAL else \"time_limit\",\n",
        "            \"objective_value\": model.ObjVal,\n",
        "            \"decision_variables\": {v.VarName: v.X for v in model.getVars()},\n",
        "        }\n",
        "        with open(baseline_result_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(baseline_result, f, indent=2)\n",
        "        model.write(str(model_lp_path))\n",
        "        model.write(str(model_mps_path))\n",
        "        print(\"Baseline saved to\", OUTPUTS_DIR)\n",
        "    else:\n",
        "        print(\"Model status:\", model.status)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Build RAG index (once)\n",
        "\n",
        "Builds `outputs/rag_index/` from the formulation .py, .lp, .mps, and data. Skip if already built."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping RAG build: index already exists at /Users/Larry.Jin/Documents/research/agent_explain/use_case/staffing_model/outputs/rag_index\n"
          ]
        }
      ],
      "source": [
        "from agentic_explain.rag.build_index import build_rag_index\n",
        "import os\n",
        "\n",
        "py_path = PROJECT_ROOT / \"use_case\" / \"staffing_model\" / \"staffing_model.py\"\n",
        "lp_path = OUTPUTS_DIR / \"model.lp\"\n",
        "mps_path = OUTPUTS_DIR / \"model.mps\"\n",
        "rag_index_dir = OUTPUTS_DIR / \"rag_index\"\n",
        "\n",
        "# Default: only rebuild if index files are missing\n",
        "REBUILD_RAG_INDEX = False  # set to True to force rebuild\n",
        "\n",
        "# To skip rebuilding if already built, check for the LlamaIndex SimpleVectorStore files\n",
        "def rag_index_exists(rag_index_dir):\n",
        "    if not rag_index_dir.exists():\n",
        "        return False\n",
        "    expected = [\n",
        "        rag_index_dir / \"docstore.json\",\n",
        "        rag_index_dir / \"default__vector_store.json\",\n",
        "        rag_index_dir / \"index_store.json\",\n",
        "    ]\n",
        "    return all(p.exists() for p in expected)\n",
        "\n",
        "if REBUILD_RAG_INDEX or not rag_index_exists(rag_index_dir):\n",
        "    build_rag_index(\n",
        "        py_path=py_path,\n",
        "        lp_path=lp_path if lp_path.exists() else None,\n",
        "        mps_path=mps_path if mps_path.exists() else None,\n",
        "        data_dir=DATA_DIR,\n",
        "        persist_dir=rag_index_dir,\n",
        "    )\n",
        "    print(\"RAG index built at\", rag_index_dir)\n",
        "else:\n",
        "    print(f\"Skipping RAG build: index already exists at {rag_index_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3a. Inspect RAG Chunks\n",
        "\n",
        "Visualize every chunk that was indexed, grouped by **source** (py, lp, mps, index_mapping).\n",
        "Each chunk is shown with its metadata and text (word-wrapped for readability)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total chunks: 18298\n",
            "\n",
            "By source:\n",
            "  index_mapping           1 chunks\n",
            "  lp                   18265 chunks\n",
            "  py                     32 chunks\n",
            "\n",
            "By source/section:\n",
            "  index_mapping/index_mapping                 1 chunks\n",
            "  lp/bounds                                   1 chunks\n",
            "  lp/constraints                           18262 chunks\n",
            "  lp/objective                                1 chunks\n",
            "  lp/variables                                1 chunks\n",
            "  py/constraints                              9 chunks\n",
            "  py/functions                                3 chunks\n",
            "  py/index_mapping                            4 chunks\n",
            "  py/objectives                               8 chunks\n",
            "  py/overview                                 1 chunks\n",
            "  py/problem_overview                         1 chunks\n",
            "  py/variables                                6 chunks\n"
          ]
        }
      ],
      "source": [
        "import textwrap\n",
        "from collections import Counter\n",
        "from agentic_explain.rag.build_index import collect_raw_chunks\n",
        "\n",
        "py_path  = PROJECT_ROOT / \"use_case\" / \"staffing_model\" / \"staffing_model.py\"\n",
        "lp_path  = OUTPUTS_DIR / \"model.lp\"\n",
        "mps_path = OUTPUTS_DIR / \"model.mps\"\n",
        "\n",
        "raw_chunks = collect_raw_chunks(\n",
        "    py_path=py_path,\n",
        "    lp_path=lp_path if lp_path.exists() else None,\n",
        "    mps_path=mps_path if mps_path.exists() else None,\n",
        "    data_dir=DATA_DIR,\n",
        ")\n",
        "\n",
        "# Summary table\n",
        "source_counts = Counter(c[\"metadata\"].get(\"source\", \"?\") for c in raw_chunks)\n",
        "section_counts = Counter(\n",
        "    f\"{c['metadata'].get('source','?')}/{c['metadata'].get('section','?')}\"\n",
        "    for c in raw_chunks\n",
        ")\n",
        "\n",
        "print(f\"Total chunks: {len(raw_chunks)}\\n\")\n",
        "print(\"By source:\")\n",
        "for src, cnt in sorted(source_counts.items()):\n",
        "    print(f\"  {src:20s} {cnt:4d} chunks\")\n",
        "print(\"\\nBy source/section:\")\n",
        "for key, cnt in sorted(section_counts.items()):\n",
        "    print(f\"  {key:40s} {cnt:4d} chunks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Showing 1 of 1 chunks (source=index_mapping)\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "Chunk 0  |  source=index_mapping  section=index_mapping\n",
            "         |  chars=991\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "Index mapping (same model in .py, .lp, .mps): j = employee index (0 to n_employees-1):   j=0: Josh\n",
            "  (Lead)   j=1: Utsav (Junior)   j=2: Shivarjun (Junior)   j=3: Nancy (Senior)   j=4: Larry\n",
            "  (Manager)   j=5: Yimin (Lead)   j=6: Minnie (Junior)   j=7: Jianchen (Senior)   j=8: Stefano\n",
            "  (Senior)   j=9: Yash (Manager)   j=10: Bhavya (Senior)   j=11: Arpit (Junior)   j=12: Jason\n",
            "  (Junior)   j=13: Sruti (Senior) d = project index (0 to n_projects-1):   d=0: PSO Base development\n",
            "  d=1: PSO v8 migration   d=2: PSO product demo   d=3: Paa PSO COE   d=4: Cgg PSO COE   d=5: Foo PSO\n",
            "  Pilot   d=6: Nuu PSO COE   d=7: Saa PSO Pilot   d=8: IO Base development   d=9: IO product demo\n",
            "  d=10: Ipp IO Pilot   d=11: Moo IO Pilot   d=12: Cmm IO Pilot   d=13: Bhh IO COE   d=14: DF Base\n",
            "  development   d=15: DF product demo   d=16: Nuu DF Pilot   d=17: Cmm DF Pilot   d=18: Hee DF COE\n",
            "  d=19: Koo DF Pilot   d=20: Tyy DF COE   d=21: Saa DF Pilot t = week index (0 to horizon-1). Use\n",
            "  'Week N' in queries for t=N.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# === Browse chunks: pick a source to sample ===\n",
        "# Change SOURCE_FILTER to inspect different sources: \"py\", \"lp\", \"mps\", \"index_mapping\", or None for all\n",
        "SOURCE_FILTER = \"index_mapping\"       # <-- change me\n",
        "MAX_DISPLAY   = 10         # how many chunks to show\n",
        "\n",
        "filtered = [c for c in raw_chunks if SOURCE_FILTER is None or c[\"metadata\"].get(\"source\") == SOURCE_FILTER]\n",
        "print(f\"Showing {min(MAX_DISPLAY, len(filtered))} of {len(filtered)} chunks (source={SOURCE_FILTER or 'all'})\\n\")\n",
        "\n",
        "for i, chunk in enumerate(filtered[:MAX_DISPLAY]):\n",
        "    meta = chunk[\"metadata\"]\n",
        "    text = chunk[\"text\"]\n",
        "    # Header\n",
        "    print(f\"{'─' * 80}\")\n",
        "    print(f\"Chunk {i}  |  source={meta.get('source')}  section={meta.get('section')}\")\n",
        "    extra_keys = {k: v for k, v in meta.items() if k not in (\"source\", \"section\", \"path\")}\n",
        "    if extra_keys:\n",
        "        print(f\"         |  {extra_keys}\")\n",
        "    print(f\"         |  chars={len(text)}\")\n",
        "    print(f\"{'─' * 80}\")\n",
        "    # Word-wrapped text (first 600 chars if very long)\n",
        "    display_text = text if len(text) <= 1000 else text[:1000] + \"\\n... [truncated]\"\n",
        "    print(textwrap.fill(display_text, width=100, subsequent_indent=\"  \"))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3b. Inspect Persisted Index: Docstore Nodes & Embedding Vectors\n",
        "\n",
        "After LlamaIndex builds the index, it sub-chunks your documents into smaller nodes and embeds each one.\n",
        "The persisted files (`docstore.json`, `default__vector_store.json`) are single-line JSON and too large to open in an IDE.\n",
        "This cell **samples** a few nodes and their embeddings so you can inspect them here.\n",
        "\n",
        "> **Note**: `docstore.json` (31 MB, 18k+ nodes) and `default__vector_store.json` (646 MB, 18k embeddings × 1536 dims) are written by LlamaIndex in compact single-line JSON. The cell below loads and pretty-prints samples without modifying the files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Docstore: 18533 nodes total\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────────────────────────\n",
            "  Node 0  id=a2b11031-293...  source=lp  section=constraints\n",
            "  chars=77  metadata_keys=['source', 'section', 'constraint_name', 'path']\n",
            "──────────────────────────────────────────────────────────────────────────────────────────\n",
            "  Constraint: indicator_constraint_0_2_1_6 - 1e+08 x[2,1,6] + x_ind[2,1,6] <= 0\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────────────────────────\n",
            "  Node 1  id=6ae5a987-820...  source=lp  section=constraints\n",
            "  chars=77  metadata_keys=['source', 'section', 'constraint_name', 'path']\n",
            "──────────────────────────────────────────────────────────────────────────────────────────\n",
            "  Constraint: indicator_constraint_0_5_0_2 - 1e+08 x[5,0,2] + x_ind[5,0,2] <= 0\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────────────────────────\n",
            "  Node 2  id=abae8f1e-4f7...  source=lp  section=constraints\n",
            "  chars=77  metadata_keys=['source', 'section', 'constraint_name', 'path']\n",
            "──────────────────────────────────────────────────────────────────────────────────────────\n",
            "  Constraint: indicator_constraint_1_9_18_20 - x[9,18,20] + x_ind[9,18,20] >= 0\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────────────────────────\n",
            "  Node 3  id=ee64300b-38f...  source=lp  section=constraints\n",
            "  chars=81  metadata_keys=['source', 'section', 'constraint_name', 'path']\n",
            "──────────────────────────────────────────────────────────────────────────────────────────\n",
            "  Constraint: indicator_constraint_0_11_22_17 - 1e+08 x[11,22,17] + x_ind[11,22,17]\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────────────────────────\n",
            "  Node 4  id=b26457a6-dbe...  source=lp  section=constraints\n",
            "  chars=77  metadata_keys=['source', 'section', 'constraint_name', 'path']\n",
            "──────────────────────────────────────────────────────────────────────────────────────────\n",
            "  Constraint: indicator_constraint_1_3_24_20 - x[3,24,20] + x_ind[3,24,20] >= 0\n",
            "\n",
            "==========================================================================================\n",
            "  Sampling from default__vector_store.json (embeddings)\n",
            "==========================================================================================\n",
            "\n",
            "Total embeddings: 18533\n",
            "Embedding dimension: 1536\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────────────────────────\n",
            "  Embedding 0  id=c0ce6896-64e...\n",
            "  ref_doc_id=5206cf5a-5ba...  metadata={'source': 'lp', 'section': 'constraints', 'constraint_name': 'indicator_constraint_0_11_1_0', 'path': '/Users/Larry.Jin/Documents/research/agent_explain/use_case/staffing_model/outputs/model.lp', '_node_type': 'TextNode', 'document_id': '5206cf5a-5baa-4c90-b4e6-79afafc1e1e5', 'doc_id': '5206cf5a-5baa-4c90-b4e6-79afafc1e1e5', 'ref_doc_id': '5206cf5a-5baa-4c90-b4e6-79afafc1e1e5'}\n",
            "  vector (first 8 dims): [0.012165, 0.008683, -0.008941, -0.013669, 0.018956, 0.022954, -0.032096, 0.007515]...\n",
            "  vector (last  4 dims): ...[-0.000196, 0.001993, -0.009227, -0.027195]\n",
            "  text: Constraint: indicator_constraint_0_11_1_0\n",
            "- 1e+08 x[11,1,0] + x_ind[11,1,0] <= 0\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────────────────────────\n",
            "  Embedding 1  id=b71b10de-dc2...\n",
            "  ref_doc_id=7ab1bd16-901...  metadata={'source': 'lp', 'section': 'constraints', 'constraint_name': 'indicator_constraint_0_3_18_4', 'path': '/Users/Larry.Jin/Documents/research/agent_explain/use_case/staffing_model/outputs/model.lp', '_node_type': 'TextNode', 'document_id': '7ab1bd16-901e-4b07-9d0d-1fffcec95b2e', 'doc_id': '7ab1bd16-901e-4b07-9d0d-1fffcec95b2e', 'ref_doc_id': '7ab1bd16-901e-4b07-9d0d-1fffcec95b2e'}\n",
            "  vector (first 8 dims): [0.011902, 0.007043, -0.006982, -0.012102, 0.016427, 0.023302, -0.028243, 0.004082]...\n",
            "  vector (last  4 dims): ...[-0.002868, 0.005213, -0.00903, -0.023975]\n",
            "  text: Constraint: indicator_constraint_0_3_18_4\n",
            "- 1e+08 x[3,18,4] + x_ind[3,18,4] <= 0\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────────────────────────\n",
            "  Embedding 2  id=1b658b80-7c5...\n",
            "  ref_doc_id=fe91e347-d3e...  metadata={'source': 'lp', 'section': 'constraints', 'constraint_name': 'indicator_constraint_0_2_16_1', 'path': '/Users/Larry.Jin/Documents/research/agent_explain/use_case/staffing_model/outputs/model.lp', '_node_type': 'TextNode', 'document_id': 'fe91e347-d3e5-44e1-92cd-156a69d26551', 'doc_id': 'fe91e347-d3e5-44e1-92cd-156a69d26551', 'ref_doc_id': 'fe91e347-d3e5-44e1-92cd-156a69d26551'}\n",
            "  vector (first 8 dims): [0.009497, 0.009497, -0.005994, -0.010304, 0.014919, 0.022033, -0.027952, 0.009152]...\n",
            "  vector (last  4 dims): ...[-0.004119, 0.003481, -0.009058, -0.023603]\n",
            "  text: Constraint: indicator_constraint_0_2_16_1\n",
            "- 1e+08 x[2,16,1] + x_ind[2,16,1] <= 0\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────────────────────────\n",
            "  Embedding 3  id=bea921ec-268...\n",
            "  ref_doc_id=c76a104f-87d...  metadata={'source': 'lp', 'section': 'constraints', 'constraint_name': 'indicator_constraint_0_5_24_3', 'path': '/Users/Larry.Jin/Documents/research/agent_explain/use_case/staffing_model/outputs/model.lp', '_node_type': 'TextNode', 'document_id': 'c76a104f-87d8-435b-9063-c82fb428c09c', 'doc_id': 'c76a104f-87d8-435b-9063-c82fb428c09c', 'ref_doc_id': 'c76a104f-87d8-435b-9063-c82fb428c09c'}\n",
            "  vector (first 8 dims): [0.008188, 0.010085, -0.004982, -0.013685, 0.015403, 0.022846, -0.028615, 0.004946]...\n",
            "  vector (last  4 dims): ...[-0.005511, 0.004642, -0.006961, -0.023863]\n",
            "  text: Constraint: indicator_constraint_0_5_24_3\n",
            "- 1e+08 x[5,24,3] + x_ind[5,24,3] <= 0\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────────────────────────\n",
            "  Embedding 4  id=e18e8c93-c82...\n",
            "  ref_doc_id=914be0aa-800...  metadata={'source': 'lp', 'section': 'constraints', 'constraint_name': 'indicator_constraint_0_10_21_7', 'path': '/Users/Larry.Jin/Documents/research/agent_explain/use_case/staffing_model/outputs/model.lp', '_node_type': 'TextNode', 'document_id': '914be0aa-800d-45be-a1e4-85860425b9bf', 'doc_id': '914be0aa-800d-45be-a1e4-85860425b9bf', 'ref_doc_id': '914be0aa-800d-45be-a1e4-85860425b9bf'}\n",
            "  vector (first 8 dims): [0.012095, 0.002887, -0.006402, -0.01308, 0.024389, 0.022222, -0.033793, 0.00558]...\n",
            "  vector (last  4 dims): ...[-0.008512, 0.004224, -0.005725, -0.026513]\n",
            "  text: Constraint: indicator_constraint_0_10_21_7\n",
            "- 1e+08 x[10,21,7] + x_ind[10,21,7] <= 0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import json, textwrap, random\n",
        "\n",
        "rag_dir = OUTPUTS_DIR / \"rag_index\"\n",
        "NUM_SAMPLES = 5  # how many nodes/vectors to show\n",
        "\n",
        "# ── 1. Docstore: sample text nodes ──────────────────────────────────────────\n",
        "with open(rag_dir / \"docstore.json\", \"r\") as f:\n",
        "    docstore = json.load(f)\n",
        "\n",
        "doc_data = docstore.get(\"docstore/data\", {})\n",
        "doc_ids = list(doc_data.keys())\n",
        "print(f\"Docstore: {len(doc_ids)} nodes total\\n\")\n",
        "\n",
        "sample_ids = random.sample(doc_ids, min(NUM_SAMPLES, len(doc_ids)))\n",
        "for i, nid in enumerate(sample_ids):\n",
        "    entry = doc_data[nid]\n",
        "    d = entry.get(\"__data__\", entry)\n",
        "    text = d.get(\"text\", \"\")\n",
        "    meta = d.get(\"metadata\", {})\n",
        "    source = meta.get(\"source\", \"?\")\n",
        "    section = meta.get(\"section\", \"?\")\n",
        "    print(f\"{'─' * 90}\")\n",
        "    print(f\"  Node {i}  id={nid[:12]}...  source={source}  section={section}\")\n",
        "    print(f\"  chars={len(text)}  metadata_keys={list(meta.keys())}\")\n",
        "    print(f\"{'─' * 90}\")\n",
        "    display = text if len(text) <= 400 else text[:400] + \"\\n  ... [truncated]\"\n",
        "    print(textwrap.fill(display, width=95, initial_indent=\"  \", subsequent_indent=\"  \"))\n",
        "    print()\n",
        "\n",
        "# ── 2. Vector store: sample embeddings ──────────────────────────────────────\n",
        "# Stream-parse to avoid loading 646MB into memory all at once\n",
        "# We just need a few sample keys from embedding_dict\n",
        "print(f\"{'=' * 90}\")\n",
        "print(\"  Sampling from default__vector_store.json (embeddings)\")\n",
        "print(f\"{'=' * 90}\\n\")\n",
        "\n",
        "# Load only the metadata and text_id mapping (small), and sample embedding keys\n",
        "vs_path = rag_dir / \"default__vector_store.json\"\n",
        "# Read the full file — it's large but we only extract what we need\n",
        "with open(vs_path, \"r\") as f:\n",
        "    vs_data = json.load(f)\n",
        "\n",
        "emb_dict = vs_data.get(\"embedding_dict\", {})\n",
        "text_to_doc = vs_data.get(\"text_id_to_ref_doc_id\", {})\n",
        "meta_dict = vs_data.get(\"metadata_dict\", {})\n",
        "emb_ids = list(emb_dict.keys())\n",
        "print(f\"Total embeddings: {len(emb_ids)}\")\n",
        "if emb_ids:\n",
        "    dim = len(emb_dict[emb_ids[0]])\n",
        "    print(f\"Embedding dimension: {dim}\\n\")\n",
        "\n",
        "sample_emb_ids = random.sample(emb_ids, min(NUM_SAMPLES, len(emb_ids)))\n",
        "for i, eid in enumerate(sample_emb_ids):\n",
        "    vec = emb_dict[eid]\n",
        "    ref_doc = text_to_doc.get(eid, \"?\")\n",
        "    meta = meta_dict.get(eid, {})\n",
        "    # Look up the text from docstore\n",
        "    doc_entry = doc_data.get(eid, {})\n",
        "    doc_d = doc_entry.get(\"__data__\", doc_entry) if doc_entry else {}\n",
        "    node_text = doc_d.get(\"text\", \"(not in docstore)\")\n",
        "\n",
        "    print(f\"{'─' * 90}\")\n",
        "    print(f\"  Embedding {i}  id={eid[:12]}...\")\n",
        "    print(f\"  ref_doc_id={ref_doc[:12]}...  metadata={meta}\")\n",
        "    print(f\"  vector (first 8 dims): {[round(v, 6) for v in vec[:8]]}...\")\n",
        "    print(f\"  vector (last  4 dims): ...{[round(v, 6) for v in vec[-4:]]}\")\n",
        "    text_preview = node_text if len(node_text) <= 200 else node_text[:200] + \"...\"\n",
        "    print(f\"  text: {text_preview}\")\n",
        "    print()\n",
        "\n",
        "del vs_data, emb_dict  # free memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Run the workflow\n",
        "\n",
        "Load baseline and RAG index, then run the agentic workflow for a natural-language query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "from use_case.staffing_model import build_gurobi_model\n",
        "from agentic_explain.rag.plain_rag import build_plain_rag\n",
        "from agentic_explain.workflow.graph import create_workflow, invoke_workflow\n",
        "\n",
        "# Load baseline and build Plain RAG strategy\n",
        "with open(OUTPUTS_DIR / \"baseline_result.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    baseline_result = json.load(f)\n",
        "\n",
        "rag_strategy = build_plain_rag(\n",
        "    py_path=PROJECT_ROOT / \"use_case\" / \"staffing_model\" / \"staffing_model.py\",\n",
        "    lp_path=OUTPUTS_DIR / \"model.lp\",\n",
        "    mps_path=OUTPUTS_DIR / \"model.mps\",\n",
        "    data_dir=DATA_DIR,\n",
        "    persist_dir=OUTPUTS_DIR / \"rag_index\",\n",
        ")\n",
        "openai_client = OpenAI()\n",
        "\n",
        "LLM_TEMPERATURE = 0  # 0 = deterministic; increase for more varied summaries\n",
        "\n",
        "workflow = create_workflow(\n",
        "    openai_client=openai_client,\n",
        "    rag_strategy=rag_strategy,\n",
        "    baseline_result=baseline_result,\n",
        "    data_dir=str(DATA_DIR),\n",
        "    build_model_fn=build_gurobi_model,\n",
        "    inputs=inputs,\n",
        "    env_kwargs=env_kwargs,\n",
        "    outputs_dir=str(OUTPUTS_DIR),\n",
        "    temperature=LLM_TEMPERATURE,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Example query: counterfactual (\"why not\")\n",
        "# user_query = \"Why was Josh not staffed on Ipp IO Pilot in week 6?\"\n",
        "\n",
        "# final_state = invoke_workflow(\n",
        "#     workflow,\n",
        "#     user_query,\n",
        "#     baseline_result=baseline_result,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(\"Query:\", user_query)\n",
        "# print()\n",
        "# print(\"Summary:\")\n",
        "# import textwrap\n",
        "# print(textwrap.fill(\n",
        "#     final_state.get(\"final_summary\", \"(no summary)\"), \n",
        "#     width=80\n",
        "#     ))\n",
        "\n",
        "# # Optional: print full debug (retrieval, LLM messages, applied constraints, comparison)\n",
        "# # from agentic_explain.workflow import debug\n",
        "# # debug.print_workflow_summary(final_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Evaluation Dataset\n",
        "\n",
        "Load the evaluation query dataset (`use_case/staffing_model/queries.json`) and run individual queries.\n",
        "Each query has a **reference answer** for LLM-as-judge evaluation.\n",
        "\n",
        "| Category | Count | Description |\n",
        "|----------|-------|-------------|\n",
        "| `objective / missing_demand` | 7 | Why is project X understaffed in week Y? |\n",
        "| `objective / idle_time` | 3 | Why is employee X idle in week Y? |\n",
        "| `objective / staffing_consistency` | 2 | Why is employee X on project Y? |\n",
        "| `objective / out_of_cohort_penalty` | 2 | Why is employee X (cohort A) on project Y (cohort B)? |\n",
        "| `constraint / max_concurrency` | 5 | Why is employee X not on project Y? (at concurrency limit) |\n",
        "| `constraint / demand_balance_inactive` | 4 | Why is employee X not on project Y in week Z? (project inactive) |\n",
        "| `constraint / oversight_requirement` | 2 | Oversight-related staffing questions |\n",
        "| `constraint / employee_allocation` | 1 | Capacity (100%) constraint |\n",
        "| `mixed` | 3 | Peak crunch, specific employee requirements |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 29 evaluation queries\n",
            "\n",
            "  [ 0] [F] objective  / missing_demand                Why is IO Base development understaffed in week 10?\n",
            "  [ 1] [F] objective  / missing_demand                Why is PSO Base development missing demand in week 15?\n",
            "  [ 2] [F] objective  / missing_demand                Why is Saa DF Pilot not fully staffed in week 14?\n",
            "  [ 3] [F] objective  / missing_demand                Why is DF Base development understaffed in week 16?\n",
            "  [ 4] [F] objective  / missing_demand                Why is Foo PSO Pilot understaffed in week 11?\n",
            "  [ 5] [F] objective  / missing_demand                Why is Saa PSO Pilot not fully staffed in week 19?\n",
            "  [ 6] [F] objective  / missing_demand                Why does IO Base development have unmet demand in week 20?\n",
            "  [ 7] [F] objective  / idle_time                     Why is Yimin idle in week 0?\n",
            "  [ 8] [F] objective  / idle_time                     Why is Sruti idle in week 3?\n",
            "  [ 9] [F] objective  / idle_time                     Why is Yimin idle in week 24?\n",
            "  [10] [F] objective  / staffing_consistency          Why is Larry assigned to Paa PSO COE in week 5?\n",
            "  [11] [F] objective  / staffing_consistency          Why is Jianchen working on Cmm IO Pilot in week 8?\n",
            "  [12] [F] objective  / out_of_cohort_penalty         Why is Yimin assigned to Koo DF Pilot in week 15?\n",
            "  [13] [F] objective  / out_of_cohort_penalty         Why is Bhavya working on IO Base development in week 4?\n",
            "  [14] [F] constraint / max_concurrency               Why is Utsav not staffed on PSO Base development in week 5?\n",
            "  [15] [F] constraint / max_concurrency               Why is Minnie not working on IO product demo in week 3?\n",
            "  [16] [F] constraint / max_concurrency               Why is Shivarjun not on IO Base development in week 10?\n",
            "  [17] [F] constraint / max_concurrency               Why is Arpit not on Hee DF COE in week 5?\n",
            "  [18] [F] constraint / max_concurrency               Why is Larry not also working on Saa PSO Pilot in week 12?\n",
            "  [19] [I] constraint / demand_balance_inactive       Why is Josh not on Foo PSO Pilot in week 5?\n",
            "  [20] [I] constraint / demand_balance_inactive       Why is Nancy not working on Saa PSO Pilot in week 8?\n",
            "  [21] [I] constraint / demand_balance_inactive       Why is Sruti not working on DF product demo in week 10?\n",
            "  [22] [I] constraint / demand_balance_inactive       Why is Yimin not working on Moo IO Pilot in week 5?\n",
            "  [23] [F] constraint / oversight_requirement         Why is Josh not allocated more to PSO Base development in week 12?\n",
            "  [24] [F] constraint / oversight_requirement         Why is Larry the one providing oversight on Foo PSO Pilot instead of Josh?\n",
            "  [25] [F] constraint / employee_allocation           Why is Shivarjun not helping with IO Base development in week 15?\n",
            "  [26] [F] mixed      / peak_crunch                   Why is Stefano not staffed on Foo PSO Pilot in week 12?\n",
            "  [27] [F] mixed      / peak_crunch                   Why is Jason not staffed on Nuu DF Pilot in week 14?\n",
            "  [28] [F] constraint / specific_employee_requirement  Why is Nancy not working on PSO v8 migration in week 3 instead of Foo PSO Pilot?\n"
          ]
        }
      ],
      "source": [
        "from use_case.staffing_model import STAFFING_QUERIES_PATH\n",
        "import json\n",
        "\n",
        "eval_path = STAFFING_QUERIES_PATH\n",
        "with open(eval_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    eval_queries = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(eval_queries)} evaluation queries\\n\")\n",
        "\n",
        "# Preview all queries\n",
        "for i, q in enumerate(eval_queries):\n",
        "    path_marker = \"F\" if q[\"expected_path\"] == \"feasible\" else \"I\"\n",
        "    print(f\"  [{i:2d}] [{path_marker}] {q['category']:10s} / {q['subcategory']:28s}  {q['query']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[obj1_missing_demand_01]  objective / missing_demand\n",
            "Query:    Why is IO Base development understaffed in week 10?\n",
            "Expected: path=feasible, expr=d_miss[10,8] == 0\n",
            "================================================================================\n",
            "Set parameter WLSAccessID\n",
            "Set parameter WLSSecret\n",
            "Set parameter LicenseID to value 2678051\n",
            "WLS license 2678051 - registered to C3.ai\n",
            "Set parameter TimeLimit to value 100\n",
            "Gurobi Optimizer version 13.0.1 build v13.0.1rc0 (mac64[arm] - Darwin 25.2.0 25C56)\n",
            "\n",
            "CPU model: Apple M3 Max\n",
            "Thread count: 16 physical cores, 16 logical processors, using up to 16 threads\n",
            "\n",
            "Non-default parameters:\n",
            "TimeLimit  100\n",
            "\n",
            "WLS license 2678051 - registered to C3.ai\n",
            "Optimize a model with 18263 rows, 17260 columns and 83921 nonzeros (Min)\n",
            "Model fingerprint: 0xf48b1980\n",
            "Model has 1244 linear objective coefficients\n",
            "Variable types: 8944 continuous, 8316 integer (8316 binary)\n",
            "Coefficient statistics:\n",
            "  Matrix range     [1e+00, 1e+08]\n",
            "  Objective range  [6e-01, 2e+00]\n",
            "  Bounds range     [1e+00, 1e+00]\n",
            "  RHS range        [5e-01, 6e+00]\n",
            "\n",
            "Found heuristic solution: objective 781.6652000\n",
            "Presolve removed 6419 rows and 6063 columns\n",
            "Presolve time: 0.03s\n",
            "Presolved: 11844 rows, 11197 columns, 47227 nonzeros\n",
            "Variable types: 5795 continuous, 5402 integer (5402 binary)\n",
            "\n",
            "Root relaxation: objective 1.065992e+02, 20697 iterations, 0.92 seconds (1.65 work units)\n",
            "\n",
            "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
            " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
            "\n",
            "     0     0  106.59917    0  140  781.66520  106.59917  86.4%     -    1s\n",
            "H    0     0                     640.2718667  106.59917  83.4%     -    1s\n",
            "H    0     0                     426.3605000  106.59917  75.0%     -    1s\n",
            "H    0     0                     423.3878334  106.59917  74.8%     -    1s\n",
            "H    0     0                     339.8953333  106.59917  68.6%     -    1s\n",
            "H    0     0                     189.1385000  106.59917  43.6%     -    1s\n",
            "H    0     0                     169.4078333  107.61439  36.5%     -    2s\n",
            "     0     0  107.61439    0  859  169.40783  107.61439  36.5%     -    2s\n",
            "H    0     0                     155.3146668  107.76585  30.6%     -    3s\n",
            "H    0     0                     155.3146667  107.76585  30.6%     -    3s\n",
            "H    0     0                     155.0556667  107.76585  30.5%     -    3s\n",
            "     0     0  107.76585    0  852  155.05567  107.76585  30.5%     -    3s\n",
            "H    0     0                     154.5881666  107.82638  30.2%     -    4s\n",
            "     0     0  107.82638    0  863  154.58817  107.82638  30.2%     -    4s\n",
            "H    0     0                     151.3185001  107.85107  28.7%     -    6s\n",
            "H    0     0                     151.0256666  107.85107  28.6%     -    6s\n",
            "H    0     0                     149.7800001  107.85107  28.0%     -    6s\n",
            "     0     0  107.85107    0  904  149.78000  107.85107  28.0%     -    6s\n",
            "     0     0  107.86303    0  938  149.78000  107.86303  28.0%     -    7s\n",
            "     0     0  107.87048    0  973  149.78000  107.87048  28.0%     -    7s\n",
            "     0     0  107.87184    0  968  149.78000  107.87184  28.0%     -    7s\n",
            "     0     0  107.87279    0  984  149.78000  107.87279  28.0%     -    7s\n",
            "     0     0  107.87281    0  979  149.78000  107.87281  28.0%     -    8s\n",
            "H    0     0                     149.7799999  108.50184  27.6%     -   11s\n",
            "     0     0  108.70084    0 1289  149.78000  108.70084  27.4%     -   11s\n",
            "H    0     0                     148.7088334  108.70084  26.9%     -   13s\n",
            "     0     0  109.08204    0 1517  148.70883  109.08204  26.6%     -   18s\n",
            "H    0     0                     146.8123334  109.39744  25.5%     -   21s\n",
            "     0     0  110.45537    0 1587  146.81233  110.45537  24.8%     -   21s\n",
            "H    0     0                     146.4935000  110.45537  24.6%     -   24s\n",
            "H    0     0                     146.1433333  110.45537  24.4%     -   24s\n",
            "     0     0  110.45537    0 1758  146.14333  110.45537  24.4%     -   24s\n",
            "     0     0  110.45537    0 1748  146.14333  110.45537  24.4%     -   25s\n",
            "H    0     0                     141.9666667  110.45537  22.2%     -   26s\n",
            "     0     0  110.45537    0 1812  141.96667  110.45537  22.2%     -   26s\n",
            "     0     0  110.45537    0 1812  141.96667  110.45537  22.2%     -   27s\n",
            "     0     0  110.45537    0 1765  141.96667  110.45537  22.2%     -   28s\n",
            "     0     0  110.78280    0 1808  141.96667  110.78280  22.0%     -   28s\n",
            "     0     0  110.78280    0 1803  141.96667  110.78280  22.0%     -   29s\n",
            "     0     0  110.78280    0 1807  141.96667  110.78280  22.0%     -   29s\n",
            "H    0     0                     141.9666665  111.05732  21.8%     -   39s\n",
            "H    0     0                     141.9666665  111.05732  21.8%     -   39s\n",
            "     0     0  111.05732    0 1973  141.96667  111.05732  21.8%     -   39s\n",
            "H    0     0                     139.3189998  111.59193  19.9%     -   45s\n",
            "     0     0  111.59193    0 2126  139.31900  111.59193  19.9%     -   45s\n",
            "H    0     0                     138.2580000  111.79121  19.1%     -   50s\n",
            "H    0     0                     138.2579999  111.79121  19.1%     -   50s\n",
            "H    0     0                     137.7591667  111.79121  18.9%     -   50s\n",
            "     0     0  112.59946    0 2134  137.75917  112.59946  18.3%     -   50s\n",
            "H    0     0                     137.7591665  112.59946  18.3%     -   55s\n",
            "H    0     0                     135.9571667  112.59946  17.2%     -   55s\n",
            "     0     0  112.63854    0 2145  135.95717  112.63854  17.2%     -   55s\n",
            "     0     0  112.63854    0 2125  135.95717  112.63854  17.2%     -   58s\n",
            "H    0     0                     135.5775667  112.63854  16.9%     -   60s\n",
            "     0     0  112.72910    0 2146  135.57757  112.72910  16.9%     -   60s\n",
            "H    0     0                     134.8736667  112.72910  16.4%     -   61s\n",
            "     0     0  112.72910    0 2153  134.87367  112.72910  16.4%     -   61s\n",
            "     0     0  112.72910    0 2152  134.87367  112.72910  16.4%     -   62s\n",
            "     0     0  112.78194    0 2131  134.87367  112.78194  16.4%     -   62s\n",
            "H    0     0                     134.8736664  112.78194  16.4%     -   66s\n",
            "H    0     0                     134.7204999  113.30849  15.9%     -   83s\n",
            "H    0     0                     134.5165000  113.30849  15.8%     -   83s\n",
            "H    0     0                     134.5165000  113.30849  15.8%     -   83s\n",
            "H    0     0                     134.3823000  113.30849  15.7%     -   83s\n",
            "     0     0  113.33702    0 2239  134.38230  113.33702  15.7%     -   83s\n",
            "H    0     0                     133.9713000  113.62179  15.2%     -   90s\n",
            "H    0     0                     133.1854999  113.62179  14.7%     -   90s\n",
            "     0     0  113.62179    0 2360  133.18550  113.62179  14.7%     -   90s\n",
            "H    0     0                     132.3263333  113.88600  13.9%     -   96s\n",
            "     0     0  113.88600    0 2419  132.32633  113.88600  13.9%     -   96s\n",
            "H    0     0                     132.2516667  114.05310  13.8%     -  102s\n",
            "H    0     0                     131.8688334  114.05310  13.5%     -  102s\n",
            "H    0     0                     131.8528333  114.05310  13.5%     -  102s\n",
            "     0     0  114.05310    0 2155  131.85283  114.05310  13.5%     -  102s\n",
            "\n",
            "Cutting planes:\n",
            "  Learned: 2\n",
            "  Gomory: 1\n",
            "  Lift-and-project: 1\n",
            "  Cover: 2\n",
            "  Implied bound: 1386\n",
            "  Clique: 933\n",
            "  MIR: 435\n",
            "  Flow cover: 1493\n",
            "  RLT: 27\n",
            "  Relax-and-lift: 70\n",
            "  BQP: 4\n",
            "  PSD: 97\n",
            "\n",
            "Explored 1 nodes (242559 simplex iterations) in 102.46 seconds (168.83 work units)\n",
            "Thread count was 16 (of 16 available processors)\n",
            "\n",
            "Solution count 10: 131.853 131.869 132.252 ... 134.72\n",
            "\n",
            "Time limit reached\n",
            "Best objective 1.318528332710e+02, best bound 1.140530964370e+02, gap 13.4997%\n"
          ]
        }
      ],
      "source": [
        "# === Pick a query by index and run the workflow ===\n",
        "QUERY_INDEX = 0  # <-- change this to run a different query\n",
        "\n",
        "q = eval_queries[QUERY_INDEX]\n",
        "print(f\"[{q['id']}]  {q['category']} / {q['subcategory']}\")\n",
        "print(f\"Query:    {q['query']}\")\n",
        "print(f\"Expected: path={q['expected_path']}, expr={q['expected_constraint_expr']}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "final_state = invoke_workflow(\n",
        "    workflow,\n",
        "    q[\"query\"],\n",
        "    baseline_result=baseline_result,\n",
        ")\n",
        "\n",
        "actual_path = final_state.get(\"counterfactual_status\", \"unknown\")\n",
        "actual_answer = final_state.get(\"final_summary\", \"(no summary)\")\n",
        "actual_exprs = final_state.get(\"constraint_expressions\", [])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- ACTUAL ---\n",
            "Path:        feasible  MATCH\n",
            "Constraints: ['d_miss[10,8] == 0']\n",
            "Answer:                   The user's change resulted in a slight worsening of the total objective by\n",
            "             approximately 1.6%, increasing from 129.81 to 131.85. The most significant changes\n",
            "             were observed in the cost of missing demand, which increased by 3.8%, and idle\n",
            "             time, which rose by 3.1%. This indicates that while some staffing adjustments may\n",
            "             have improved project allocations, they also led to higher unmet demand and\n",
            "             increased idle time for employees. The trade-off here is that although the out-of-\n",
            "             cohort penalty decreased significantly (by 50%), the overall impact was negative\n",
            "             due to the increased costs associated with unmet staffing needs and idle\n",
            "             resources.\n",
            "\n",
            "--- REFERENCE ---\n",
            "Path:        feasible\n",
            "Constraint:  d_miss[10,8] == 0\n",
            "Theme:       trade-off: low understaffing cost vs higher-cost projects\n",
            "Answer:                   IO Base development (d=8) has the lowest understaffing penalty in the model\n",
            "             (0.6-0.75, linearly increasing). In week 10, the optimizer leaves 1.5 FTE of\n",
            "             demand unmet because all available employees are allocated to projects with higher\n",
            "             understaffing costs. Week 10 is when Foo PSO Pilot (cost 0.6-1.1) starts and\n",
            "             overlaps with IO product demo (cost 1.3-1.1), Cmm IO Pilot (cost 1.0), and several\n",
            "             COE projects. Forcing d_miss[10,8]=0 would require pulling employees from those\n",
            "             higher-cost projects, increasing total objective cost. The optimizer sacrifices IO\n",
            "             Base development (only 30.6% of demand met overall) to minimize total penalty.\n"
          ]
        }
      ],
      "source": [
        "import textwrap\n",
        "\n",
        "print(f\"\\n--- ACTUAL ---\")\n",
        "print(f\"Path:        {actual_path}  {'MATCH' if actual_path == q['expected_path'] else 'MISMATCH'}\")\n",
        "print(f\"Constraints: {actual_exprs}\")\n",
        "wrapped_actual_answer = textwrap.fill(actual_answer, width=95, initial_indent=\"             \", subsequent_indent=\"             \") if actual_answer else actual_answer\n",
        "print(f\"Answer:      {wrapped_actual_answer}\")\n",
        "print(f\"\\n--- REFERENCE ---\")\n",
        "print(f\"Path:        {q['expected_path']}\")\n",
        "print(f\"Constraint:  {q['expected_constraint_expr']}\")\n",
        "print(f\"Theme:       {q['expected_answer_theme']}\")\n",
        "wrapped_reference_answer = textwrap.fill(q['reference_answer'], width=95, initial_indent=\"             \", subsequent_indent=\"             \") if q.get('reference_answer') else q['reference_answer']\n",
        "print(f\"Answer:      {wrapped_reference_answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Debug & Evaluation\n",
        "\n",
        "For **quick debug**: uncomment and run `from agentic_explain.workflow import debug; debug.print_workflow_summary(final_state)` in the cell above to print retrieval, LLM messages, applied constraints, and comparison in one go.\n",
        "\n",
        "For **comparing Plain RAG vs Graph RAG vs No-RAG**, use `notebooks/RAGComparison.ipynb`.\n",
        "\n",
        "### 5a. Retrieval Debug: Retrieved Chunks & Scores\n",
        "\n",
        "After running a query above, inspect which RAG chunks were retrieved and their relevance scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==========================================================================================\n",
            "  Stage: constraint_generation\n",
            "  Retrieval query: Force d=8 to be adequately staffed in t=10.\n",
            "  Top-k: 5\n",
            "==========================================================================================\n",
            "\n",
            "  ── Chunk 0  score=0.8034\n",
            "     source=lp  section=constraints\n",
            "     {'constraint_name': 'staffed_indicator_0_8_10'}\n",
            "     Constraint: staffed_indicator_0_8_10 - x[8,0,10] - x[8,1,10] - x[8,2,10] - x[8,3,10]\n",
            "\n",
            "  ── Chunk 1  score=0.8032\n",
            "     source=py  section=constraints\n",
            "     {'constraint_name': 'demand_balance'}\n",
            "     ### demand_balance Name: Demand Balance. For each project d and week t: sum_j x[j,t,d] *\n",
            "     F_j + d_miss[t,d] = D[d,t]. Description: Total staffing (allocations weighted by employee\n",
            "     FTE) plus unmet demand equals required demand. Business context: Core balance of supply\n",
            "     and demand. If we cannot meet demand, d_miss captures the shortage. Variables: x, d_miss.\n",
            "\n",
            "  ── Chunk 2  score=0.8030\n",
            "     source=lp  section=constraints\n",
            "     {'constraint_name': 'staffed_indicator_1_8_10'}\n",
            "     Constraint: staffed_indicator_1_8_10 - x[8,0,10] - x[8,1,10] - x[8,2,10] - x[8,3,10]\n",
            "\n",
            "  ── Chunk 3  score=0.8028\n",
            "     source=lp  section=constraints\n",
            "     {'constraint_name': 'staffed_indicator_0_8_8'}\n",
            "     Constraint: staffed_indicator_0_8_8 - x[8,0,8] - x[8,1,8] - x[8,2,8] - x[8,3,8]\n",
            "\n",
            "  ── Chunk 4  score=0.8027\n",
            "     source=lp  section=constraints\n",
            "     {'constraint_name': 'staffed_indicator_1_8_8'}\n",
            "     Constraint: staffed_indicator_1_8_8 - x[8,0,8] - x[8,1,8] - x[8,2,8] - x[8,3,8]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import textwrap\n",
        "\n",
        "rag_debug = final_state.get(\"rag_retrieval_debug\", {})\n",
        "\n",
        "for stage_name, info in rag_debug.items():\n",
        "    if stage_name == \"strategy\" or not isinstance(info, dict):\n",
        "        continue\n",
        "    print(f\"{'=' * 90}\")\n",
        "    print(f\"  Stage: {stage_name}\")\n",
        "    print(f\"  Retrieval query: {info.get('query', '?')}\")\n",
        "    print(f\"  Top-k: {info.get('top_k', '?')}\")\n",
        "    if \"iis_constraint_names\" in info:\n",
        "        print(f\"  IIS constraints: {info['iis_constraint_names']}\")\n",
        "    print(f\"{'=' * 90}\")\n",
        "\n",
        "    for i, chunk in enumerate(info.get(\"chunks\", [])):\n",
        "        score = chunk.get(\"score\")\n",
        "        meta = chunk.get(\"metadata\", {})\n",
        "        text = chunk.get(\"text\", \"\")\n",
        "        print(f\"\\n  ── Chunk {i}  score={score:.4f}\" if score is not None else f\"\\n  ── Chunk {i}  score=N/A\")\n",
        "        print(f\"     source={meta.get('source', '?')}  section={meta.get('section', '?')}\")\n",
        "        extra = {k: v for k, v in meta.items() if k not in (\"source\", \"section\", \"path\")}\n",
        "        if extra:\n",
        "            print(f\"     {extra}\")\n",
        "        # Show text (truncated & wrapped)\n",
        "        display = text if len(text) <= 500 else text[:500] + \"\\n     ... [truncated]\"\n",
        "        wrapped = textwrap.fill(display, width=95, initial_indent=\"     \", subsequent_indent=\"     \")\n",
        "        print(wrapped)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5b. LLM Messages Debug\n",
        "\n",
        "The exact system + user messages sent to the LLM at each RAG-augmented stage, and the raw response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==========================================================================================\n",
            "  Stage: constraint_generation\n",
            "==========================================================================================\n",
            "\n",
            "  [SYSTEM MESSAGE]\n",
            "  You translate a user request into one or more constraint expressions for a Gurobi\n",
            "  optimization model. Available decision variables (use exact names): d_miss, x, x_idle, x_ind,\n",
            "  x_p_ind. Format: variable_name[index1,index2,...] == value or >= value or <= value. Value can\n",
            "  be a number (e.g. 0, 1) or a data parameter (e.g. D[t,d] for demand). You may use a single\n",
            "  variable or a sum of variables (e.g. x[j,t,d] + x[j+1,t,d] + ... >= D[d,t]). Example format:\n",
            "  d_miss[0,0] == 1, x[0,0] == 1, x_idle[0,0] == 1. For 'force no unmet demand': d_miss[t,d] ==\n",
            "  0. Use the RAG context below to understand variable dimensions and index meanings. Output\n",
            "  only the constraint line(s), one per line, no explanation.\n",
            "\n",
            "  [USER MESSAGE]  (first 3000 chars)\n",
            "  RAG context: Constraint: staffed_indicator_0_8_10 - x[8,0,10] - x[8,1,10] - x[8,2,10] -\n",
            "  x[8,3,10] ### demand_balance Name: Demand Balance. For each project d and week t: sum_j\n",
            "  x[j,t,d] * F_j + d_miss[t,d] = D[d,t]. Description: Total staffing (allocations weighted by\n",
            "  employee FTE) plus unmet demand equals required demand. Business context: Core balance of\n",
            "  supply and demand. If we cannot meet demand, d_miss captures the shortage. Variables: x,\n",
            "  d_miss. Constraint: staffed_indicator_1_8_10 - x[8,0,10] - x[8,1,10] - x[8,2,10] - x[8,3,10]\n",
            "  Constraint: staffed_indicator_0_8_8 - x[8,0,8] - x[8,1,8] - x[8,2,8] - x[8,3,8] Constraint:\n",
            "  staffed_indicator_1_8_8 - x[8,0,8] - x[8,1,8] - x[8,2,8] - x[8,3,8]  User request: Force d=8\n",
            "  to be adequately staffed in t=10.\n",
            "\n",
            "  [RAW LLM RESPONSE]\n",
            "  d_miss[10,8] == 0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "CHARS_TO_SHOW = 3000\n",
        "llm_debug = final_state.get(\"llm_messages_debug\", {})\n",
        "\n",
        "for stage_name, info in llm_debug.items():\n",
        "    print(f\"{'=' * 90}\")\n",
        "    print(f\"  Stage: {stage_name}\")\n",
        "    print(f\"{'=' * 90}\")\n",
        "\n",
        "    print(f\"\\n  [SYSTEM MESSAGE]\")\n",
        "    sys_msg = info.get(\"system\", \"\")\n",
        "    print(textwrap.fill(sys_msg, width=95, initial_indent=\"  \", subsequent_indent=\"  \"))\n",
        "\n",
        "    print(f\"\\n  [USER MESSAGE]  (first {CHARS_TO_SHOW} chars)\")\n",
        "    user_msg = info.get(\"user\", \"\")\n",
        "    display_user = user_msg if len(user_msg) <= CHARS_TO_SHOW else user_msg[:CHARS_TO_SHOW] + \"\\n  ... [truncated]\"\n",
        "    print(textwrap.fill(display_user, width=95, initial_indent=\"  \", subsequent_indent=\"  \"))\n",
        "\n",
        "    print(f\"\\n  [RAW LLM RESPONSE]\")\n",
        "    raw = info.get(\"raw_response\", \"\")\n",
        "    print(textwrap.fill(raw, width=95, initial_indent=\"  \", subsequent_indent=\"  \"))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5c. Constraint Parsing Debug\n",
        "\n",
        "Shows how the raw LLM response was parsed into constraint expressions:\n",
        "regex matches, which passed/failed variable-name validation, and why."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==========================================================================================\n",
            "  Constraint Parsing Debug\n",
            "==========================================================================================\n",
            "\n",
            "  Raw LLM response (attempt 1):\n",
            "    d_miss[10,8] == 0\n",
            "\n",
            "  Regex matches (attempt 1): ['d_miss[10,8] == 0']\n",
            "  Valid expressions: ['d_miss[10,8] == 0']\n",
            "  Rejected expressions: []\n",
            "\n",
            "  Final constraint_expressions in state: ['d_miss[10,8] == 0']\n"
          ]
        }
      ],
      "source": [
        "llm_debug = final_state.get(\"llm_messages_debug\", {})\n",
        "cg_debug = llm_debug.get(\"constraint_generation\", {})\n",
        "parsing = cg_debug.get(\"parsing\", {})\n",
        "\n",
        "print(\"=\" * 90)\n",
        "print(\"  Constraint Parsing Debug\")\n",
        "print(\"=\" * 90)\n",
        "\n",
        "print(f\"\\n  Raw LLM response (attempt 1):\")\n",
        "print(f\"    {cg_debug.get('raw_response', '(not captured)')}\")\n",
        "\n",
        "print(f\"\\n  Regex matches (attempt 1): {parsing.get('regex_matches_attempt1', parsing.get('regex_matches', '(not captured)'))}\")\n",
        "print(f\"  Valid expressions: {parsing.get('valid_expressions', '(not captured)')}\")\n",
        "print(f\"  Rejected expressions: {parsing.get('rejected_expressions', '(not captured)')}\")\n",
        "if parsing.get(\"rejected_reason\"):\n",
        "    print(f\"  Rejection reason: {parsing['rejected_reason']}\")\n",
        "\n",
        "if parsing.get(\"retry_raw_response\") is not None:\n",
        "    print(f\"\\n  ** RETRY triggered (attempt 1 produced no valid expressions) **\")\n",
        "    print(f\"  Retry LLM response:\")\n",
        "    print(f\"    {parsing['retry_raw_response']}\")\n",
        "    print(f\"  Regex matches (retry): {parsing.get('regex_matches_retry', [])}\")\n",
        "\n",
        "if parsing.get(\"parse_failure\"):\n",
        "    print(f\"\\n  ** PARSE FAILURE (even after retry) **\")\n",
        "    print(f\"    {parsing['parse_failure']}\")\n",
        "\n",
        "print(f\"\\n  Final constraint_expressions in state: {final_state.get('constraint_expressions', '(key missing)')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5d. Applied Constraints Debug\n",
        "\n",
        "Which constraints were actually added to the Gurobi model, and what were the baseline values of those variables?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==========================================================================================\n",
            "  1 constraint(s) added to the counterfactual Gurobi model\n",
            "==========================================================================================\n",
            "\n",
            "  Constraint 0:\n",
            "    Expression:      d_miss[10,8] == 0\n",
            "    Gurobi var:      d_miss[10,8]\n",
            "    Forced value:    0.0  (forcing DOWN from 1.5000)\n",
            "    Baseline value:  1.5000\n",
            "    Var type:        C  bounds=[0.0, inf]\n",
            "    Constr name:     user_constr_d_miss_10_8\n",
            "\n",
            "  Counterfactual status: feasible\n",
            "  Baseline obj:          129.8083\n",
            "  Counterfactual obj:    131.8528\n",
            "  Delta:                 +2.0445\n"
          ]
        }
      ],
      "source": [
        "cf_result = final_state.get(\"counterfactual_result\", {})\n",
        "applied = cf_result.get(\"applied_constraints\", [])\n",
        "\n",
        "if not applied:\n",
        "    print(\"No constraints were applied (check counterfactual_result for errors).\")\n",
        "    if cf_result.get(\"error\"):\n",
        "        print(f\"  Error: {cf_result['error']}\")\n",
        "else:\n",
        "    print(f\"{'=' * 90}\")\n",
        "    print(f\"  {len(applied)} constraint(s) added to the counterfactual Gurobi model\")\n",
        "    print(f\"{'=' * 90}\")\n",
        "    for i, ac in enumerate(applied):\n",
        "        baseline_val = ac.get(\"baseline_value\")\n",
        "        forced_val = ac.get(\"forced_value\")\n",
        "        bv_str = f\"{baseline_val:.4f}\" if baseline_val is not None else \"N/A\"\n",
        "        direction = \"\"\n",
        "        if baseline_val is not None:\n",
        "            if abs(forced_val - baseline_val) < 1e-8:\n",
        "                direction = \"(no change from baseline)\"\n",
        "            elif forced_val > baseline_val:\n",
        "                direction = f\"(forcing UP from {bv_str})\"\n",
        "            else:\n",
        "                direction = f\"(forcing DOWN from {bv_str})\"\n",
        "\n",
        "        print(f\"\\n  Constraint {i}:\")\n",
        "        print(f\"    Expression:      {ac.get('expr')}\")\n",
        "        print(f\"    Gurobi var:      {ac.get('gurobi_var_name')}\")\n",
        "        print(f\"    Forced value:    {forced_val}  {direction}\")\n",
        "        print(f\"    Baseline value:  {bv_str}\")\n",
        "        print(f\"    Var type:        {ac.get('var_type')}  bounds=[{ac.get('var_lb')}, {ac.get('var_ub')}]\")\n",
        "        print(f\"    Constr name:     {ac.get('constraint_name')}\")\n",
        "\n",
        "    print(f\"\\n  Counterfactual status: {final_state.get('counterfactual_status')}\")\n",
        "    cf_obj = cf_result.get(\"objective_value\")\n",
        "    base_obj = final_state.get(\"baseline_result\", {}).get(\"objective_value\")\n",
        "    if cf_obj is not None and base_obj is not None:\n",
        "        print(f\"  Baseline obj:          {base_obj:.4f}\")\n",
        "        print(f\"  Counterfactual obj:    {cf_obj:.4f}\")\n",
        "        print(f\"  Delta:                 {cf_obj - base_obj:+.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5e. Detailed Objective Comparison & Variable Changes\n",
        "\n",
        "The compare node produces a structured breakdown of all four objective terms (baseline vs counterfactual)\n",
        "and highlights which variables changed to cause the differences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== OBJECTIVE COMPARISON ===\n",
            "Term                                    Baseline     Counter.        Delta    %Change\n",
            "─────────────────────────────────────────────────────────────────────────────────────\n",
            "cost_of_missing_demand                   56.0750      58.2195      +2.1445      +3.8%\n",
            "  (Weighted sum of unmet staffing demand across all projects and weeks)\n",
            "idle_time                                28.7333      29.6333      +0.9000      +3.1%\n",
            "  (Total employee idle time (FTE-weeks not assigned to any project))\n",
            "staffing_consistency                     43.0000      43.0000      -0.0000      -0.0%\n",
            "  (Number of unique employee-project pairings (fewer = less context switching))\n",
            "out_of_cohort_penalty                     2.0000       1.0000      -1.0000     -50.0%\n",
            "  (Penalty for assigning employees to projects outside their preferred cohort)\n",
            "─────────────────────────────────────────────────────────────────────────────────────\n",
            "TOTAL                                   129.8083     131.8528      +2.0445      +1.6%\n",
            "\n",
            "=== VARIABLE CHANGES BY FAMILY ===\n",
            "\n",
            "d_miss — 56 variables changed (top 15 by magnitude):\n",
            "  ▼ d_miss[10,8]: 1.5000 → 0.0000 (-1.5000)\n",
            "  ▲ d_miss[10,12]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▲ d_miss[11,14]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▼ d_miss[20,14]: 1.0000 → 0.0000 (-1.0000)\n",
            "  ▲ d_miss[20,21]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▲ d_miss[22,21]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▲ d_miss[23,21]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▲ d_miss[7,8]: 0.2500 → 1.0000 (+0.7500)\n",
            "  ▼ d_miss[7,14]: 0.7500 → 0.0000 (-0.7500)\n",
            "  ▼ d_miss[11,5]: 1.0000 → 0.2500 (-0.7500)\n",
            "  ▼ d_miss[22,8]: 1.5000 → 0.7500 (-0.7500)\n",
            "  ▼ d_miss[23,8]: 1.5000 → 0.7500 (-0.7500)\n",
            "  ▼ d_miss[6,0]: 0.5000 → 0.0000 (-0.5000)\n",
            "  ▼ d_miss[8,0]: 0.5000 → 0.0000 (-0.5000)\n",
            "  ▲ d_miss[8,8]: 1.0000 → 1.5000 (+0.5000)\n",
            "  ... and 41 more\n",
            "\n",
            "x — 582 variables changed (top 15 by magnitude):\n",
            "  ▲ x[1,0,4]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▼ x[1,0,6]: 1.0000 → 0.0000 (-1.0000)\n",
            "  ▲ x[1,1,4]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▼ x[1,1,6]: 1.0000 → 0.0000 (-1.0000)\n",
            "  ▲ x[1,2,4]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▼ x[1,2,6]: 1.0000 → 0.0000 (-1.0000)\n",
            "  ▲ x[1,3,4]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▼ x[1,3,6]: 1.0000 → 0.0000 (-1.0000)\n",
            "  ▲ x[1,4,4]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▼ x[1,4,6]: 1.0000 → 0.0000 (-1.0000)\n",
            "  ▲ x[1,5,4]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▼ x[1,5,6]: 1.0000 → 0.0000 (-1.0000)\n",
            "  ▲ x[1,6,4]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▼ x[1,6,6]: 1.0000 → 0.0000 (-1.0000)\n",
            "  ▲ x[1,7,4]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ... and 567 more\n",
            "\n",
            "x_idle — 24 variables changed (top 15 by magnitude):\n",
            "  ▼ x_idle[1,24]: 1.0000 → 0.0000 (-1.0000)\n",
            "  ▼ x_idle[1,25]: 1.0000 → 0.0000 (-1.0000)\n",
            "  ▲ x_idle[2,24]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▲ x_idle[2,25]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▼ x_idle[5,0]: 0.5000 → 0.0000 (-0.5000)\n",
            "  ▲ x_idle[13,11]: 0.0000 → 0.4000 (+0.4000)\n",
            "  ▲ x_idle[0,0]: 0.0000 → 0.3333 (+0.3333)\n",
            "  ▲ x_idle[9,2]: 0.0000 → 0.2000 (+0.2000)\n",
            "  ▲ x_idle[9,1]: 0.0000 → 0.2000 (+0.2000)\n",
            "  ▲ x_idle[13,0]: 0.2000 → 0.4000 (+0.2000)\n",
            "  ▲ x_idle[10,3]: 0.0000 → 0.2000 (+0.2000)\n",
            "  ▲ x_idle[9,3]: 0.0000 → 0.2000 (+0.2000)\n",
            "  ▼ x_idle[13,3]: 0.2000 → 0.0000 (-0.2000)\n",
            "  ▼ x_idle[13,6]: 0.2000 → 0.0000 (-0.2000)\n",
            "  ▲ x_idle[9,5]: 0.0000 → 0.2000 (+0.2000)\n",
            "  ... and 9 more\n",
            "\n",
            "x_ind — 453 variables changed (top 15 by magnitude):\n",
            "  ▼ x_ind[0,0,2]: 1.0000 → 0.0000 (-1.0000)\n",
            "  ▼ x_ind[0,1,1]: 1.0000 → -0.0000 (-1.0000)\n",
            "  ▼ x_ind[0,4,1]: 1.0000 → -0.0000 (-1.0000)\n",
            "  ▼ x_ind[0,6,1]: 1.0000 → -0.0000 (-1.0000)\n",
            "  ▼ x_ind[0,7,1]: 1.0000 → -0.0000 (-1.0000)\n",
            "  ▼ x_ind[0,8,1]: 1.0000 → -0.0000 (-1.0000)\n",
            "  ▼ x_ind[0,9,1]: 1.0000 → -0.0000 (-1.0000)\n",
            "  ▲ x_ind[0,10,1]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▲ x_ind[0,11,1]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▲ x_ind[0,19,7]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▼ x_ind[0,20,1]: 1.0000 → -0.0000 (-1.0000)\n",
            "  ▼ x_ind[0,21,1]: 1.0000 → -0.0000 (-1.0000)\n",
            "  ▲ x_ind[1,0,4]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▼ x_ind[1,0,6]: 1.0000 → -0.0000 (-1.0000)\n",
            "  ▲ x_ind[1,1,4]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ... and 438 more\n",
            "\n",
            "x_p_ind — 32 variables changed (top 15 by magnitude):\n",
            "  ▲ x_p_ind[1,4]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▼ x_p_ind[1,6]: 1.0000 → -0.0000 (-1.0000)\n",
            "  ▲ x_p_ind[2,1]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▼ x_p_ind[2,4]: 1.0000 → -0.0000 (-1.0000)\n",
            "  ▲ x_p_ind[2,8]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▲ x_p_ind[2,12]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▼ x_p_ind[3,1]: 1.0000 → -0.0000 (-1.0000)\n",
            "  ▲ x_p_ind[3,6]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▲ x_p_ind[4,2]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▼ x_p_ind[4,3]: 1.0000 → -0.0000 (-1.0000)\n",
            "  ▼ x_p_ind[5,19]: 1.0000 → -0.0000 (-1.0000)\n",
            "  ▲ x_p_ind[7,9]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▼ x_p_ind[7,10]: 1.0000 → -0.0000 (-1.0000)\n",
            "  ▲ x_p_ind[8,6]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▼ x_p_ind[8,11]: 1.0000 → -0.0000 (-1.0000)\n",
            "  ... and 17 more\n"
          ]
        }
      ],
      "source": [
        "# Print the full comparison summary (generated by the compare node)\n",
        "comparison = final_state.get(\"comparison_summary\", \"(no comparison)\")\n",
        "print(comparison)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === (Optional) Batch run: evaluate all queries and collect results ===\n",
        "# Uncomment and run to evaluate the full dataset.\n",
        "# Results are saved to outputs/eval_results.json for later analysis.\n",
        "\n",
        "# eval_results = []\n",
        "# for i, q in enumerate(eval_queries):\n",
        "#     print(f\"\\n[{i}/{len(eval_queries)}] {q['id']}: {q['query'][:60]}...\")\n",
        "#     state = invoke_workflow(\n",
        "#         workflow, q[\"query\"],\n",
        "#         baseline_result=baseline_result,\n",
        "#     )\n",
        "#     eval_results.append({\n",
        "#         \"query_id\": q[\"id\"],\n",
        "#         \"query\": q[\"query\"],\n",
        "#         \"expected_path\": q[\"expected_path\"],\n",
        "#         \"actual_path\": state.get(\"counterfactual_status\", \"unknown\"),\n",
        "#         \"path_match\": state.get(\"counterfactual_status\") == q[\"expected_path\"],\n",
        "#         \"expected_constraint_expr\": q[\"expected_constraint_expr\"],\n",
        "#         \"actual_constraint_exprs\": state.get(\"constraint_expressions\", []),\n",
        "#         \"actual_answer\": state.get(\"final_summary\", \"\"),\n",
        "#         \"reference_answer\": q[\"reference_answer\"],\n",
        "#         \"expected_answer_theme\": q[\"expected_answer_theme\"],\n",
        "#     })\n",
        "#     print(f\"  Path: {state.get('counterfactual_status')} \"\n",
        "#           f\"{'MATCH' if state.get('counterfactual_status') == q['expected_path'] else 'MISMATCH'}\")\n",
        "#\n",
        "# with open(OUTPUTS_DIR / \"eval_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "#     json.dump(eval_results, f, indent=2)\n",
        "# \n",
        "# n_match = sum(1 for r in eval_results if r[\"path_match\"])\n",
        "# print(f\"\\n=== Summary ===\")\n",
        "# print(f\"Path match: {n_match}/{len(eval_results)} ({100*n_match/len(eval_results):.0f}%)\")\n",
        "# print(f\"Results saved to {OUTPUTS_DIR / 'eval_results.json'}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rag",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
