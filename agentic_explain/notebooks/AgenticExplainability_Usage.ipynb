{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Agentic Explainability Workflow – Usage\n",
        "\n",
        "This notebook runs the agentic explainability pipeline: ask a natural-language question about optimization results, get a counterfactual run and a summary (trade-offs or infeasibility conflict).\n",
        "\n",
        "**Prerequisites:**\n",
        "- `config/secrets.env` with `OPENAI_API_KEY` and `GUROBI_LICENSE_FILE=config/WLS-dev-key.lic`\n",
        "- Run the first two sections once to create the baseline and RAG index; then you can run only the workflow section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup paths and load secrets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: /Users/Larry.Jin/Documents/research/agent_explain\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Project root: directory that contains agentic_explain and use_case\n",
        "def find_project_root():\n",
        "    for start in [Path.cwd()] + list(Path.cwd().parents):\n",
        "        if (start / \"agentic_explain\").is_dir() and (start / \"use_case\").is_dir():\n",
        "            return start\n",
        "    return Path.cwd()\n",
        "\n",
        "PROJECT_ROOT = find_project_root()\n",
        "\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(PROJECT_ROOT))\n",
        "\n",
        "from config.load_secrets import load_secrets, get_gurobi_env_kwargs\n",
        "\n",
        "load_secrets()  # load from config/secrets.env or project root\n",
        "print(\"Project root:\", PROJECT_ROOT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Run baseline (once)\n",
        "\n",
        "Saves `outputs/baseline_result.json`, `outputs/model.lp`, `outputs/model.mps`. Skip this cell if you already have them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping Gurobi model run: baseline outputs already exist in /Users/Larry.Jin/Documents/research/agent_explain/outputs\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from gurobipy import GRB\n",
        "\n",
        "from use_case.staffing_model import load_raw_data, process_data, build_gurobi_model\n",
        "\n",
        "from use_case.staffing_model import STAFFING_DATA_DIR, STAFFING_OUTPUTS_DIR\n",
        "DATA_DIR = STAFFING_DATA_DIR\n",
        "OUTPUTS_DIR = STAFFING_OUTPUTS_DIR\n",
        "OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Controller: if baseline outputs already exist, skip running Gurobi\n",
        "SKIP_IF_BASELINE_EXISTS = True  # Set this to False to force rerun\n",
        "\n",
        "baseline_result_path = OUTPUTS_DIR / \"baseline_result.json\"\n",
        "model_lp_path = OUTPUTS_DIR / \"model.lp\"\n",
        "model_mps_path = OUTPUTS_DIR / \"model.mps\"\n",
        "\n",
        "raw = load_raw_data(DATA_DIR)\n",
        "inputs = process_data(\n",
        "    raw[\"fte_mapping\"],\n",
        "    raw[\"concurrent_projects\"],\n",
        "    raw[\"oversight_ds_list\"],\n",
        "    raw[\"ds_list\"],\n",
        "    raw[\"project_list\"],\n",
        ")\n",
        "env_kwargs = get_gurobi_env_kwargs()\n",
        "\n",
        "def baseline_outputs_exist():\n",
        "    return baseline_result_path.exists() and model_lp_path.exists() and model_mps_path.exists()\n",
        "\n",
        "if SKIP_IF_BASELINE_EXISTS and baseline_outputs_exist():\n",
        "    print(\"Skipping Gurobi model run: baseline outputs already exist in\", OUTPUTS_DIR)\n",
        "else:\n",
        "    model = build_gurobi_model(inputs, env_kwargs)\n",
        "    model.setParam(GRB.Param.TimeLimit, 100)\n",
        "    model.optimize()\n",
        "\n",
        "    if model.status in (GRB.OPTIMAL, GRB.TIME_LIMIT):\n",
        "        baseline_result = {\n",
        "            \"status\": \"optimal\" if model.status == GRB.OPTIMAL else \"time_limit\",\n",
        "            \"objective_value\": model.ObjVal,\n",
        "            \"decision_variables\": {v.VarName: v.X for v in model.getVars()},\n",
        "        }\n",
        "        with open(baseline_result_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(baseline_result, f, indent=2)\n",
        "        model.write(str(model_lp_path))\n",
        "        model.write(str(model_mps_path))\n",
        "        print(\"Baseline saved to\", OUTPUTS_DIR)\n",
        "    else:\n",
        "        print(\"Model status:\", model.status)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Build RAG index (once)\n",
        "\n",
        "Builds `outputs/rag_index/` from the formulation .py, .lp, .mps, and data. Skip if already built."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAG index built at /Users/Larry.Jin/Documents/research/agent_explain/outputs/rag_index\n"
          ]
        }
      ],
      "source": [
        "from agentic_explain.rag.build_index import build_rag_index\n",
        "import os\n",
        "\n",
        "py_path = PROJECT_ROOT / \"use_case\" / \"staffing_model\" / \"staffing_model.py\"\n",
        "lp_path = OUTPUTS_DIR / \"model.lp\"\n",
        "mps_path = OUTPUTS_DIR / \"model.mps\"\n",
        "rag_index_dir = OUTPUTS_DIR / \"rag_index\"\n",
        "\n",
        "# Default: only rebuild if index files are missing\n",
        "REBUILD_RAG_INDEX = True  # set to True to force rebuild\n",
        "\n",
        "# To skip rebuilding if already built, check for the LlamaIndex SimpleVectorStore files\n",
        "def rag_index_exists(rag_index_dir):\n",
        "    if not rag_index_dir.exists():\n",
        "        return False\n",
        "    expected = [\n",
        "        rag_index_dir / \"docstore.json\",\n",
        "        rag_index_dir / \"default__vector_store.json\",\n",
        "        rag_index_dir / \"index_store.json\",\n",
        "    ]\n",
        "    return all(p.exists() for p in expected)\n",
        "\n",
        "if REBUILD_RAG_INDEX or not rag_index_exists(rag_index_dir):\n",
        "    build_rag_index(\n",
        "        py_path=py_path,\n",
        "        lp_path=lp_path if lp_path.exists() else None,\n",
        "        mps_path=mps_path if mps_path.exists() else None,\n",
        "        data_dir=DATA_DIR,\n",
        "        persist_dir=rag_index_dir,\n",
        "    )\n",
        "    print(\"RAG index built at\", rag_index_dir)\n",
        "else:\n",
        "    print(f\"Skipping RAG build: index already exists at {rag_index_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3a. Inspect RAG Chunks\n",
        "\n",
        "Visualize every chunk that was indexed, grouped by **source** (py, lp, mps, index_mapping).\n",
        "Each chunk is shown with its metadata and text (word-wrapped for readability)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total chunks: 18301\n",
            "\n",
            "By source:\n",
            "  index_mapping           1 chunks\n",
            "  lp                   18265 chunks\n",
            "  py                     35 chunks\n",
            "\n",
            "By source/section:\n",
            "  index_mapping/index_mapping                 1 chunks\n",
            "  lp/bounds                                   1 chunks\n",
            "  lp/constraints                           18262 chunks\n",
            "  lp/objective                                1 chunks\n",
            "  lp/variables                                1 chunks\n",
            "  py/constraints                             11 chunks\n",
            "  py/functions                                3 chunks\n",
            "  py/index_mapping                            4 chunks\n",
            "  py/objectives                               9 chunks\n",
            "  py/overview                                 1 chunks\n",
            "  py/problem_overview                         1 chunks\n",
            "  py/variables                                6 chunks\n"
          ]
        }
      ],
      "source": [
        "import textwrap\n",
        "from collections import Counter\n",
        "from agentic_explain.rag.build_index import collect_raw_chunks\n",
        "\n",
        "py_path  = PROJECT_ROOT / \"use_case\" / \"staffing_model\" / \"staffing_model.py\"\n",
        "lp_path  = OUTPUTS_DIR / \"model.lp\"\n",
        "mps_path = OUTPUTS_DIR / \"model.mps\"\n",
        "\n",
        "raw_chunks = collect_raw_chunks(\n",
        "    py_path=py_path,\n",
        "    lp_path=lp_path if lp_path.exists() else None,\n",
        "    mps_path=mps_path if mps_path.exists() else None,\n",
        "    data_dir=DATA_DIR,\n",
        ")\n",
        "\n",
        "# Summary table\n",
        "source_counts = Counter(c[\"metadata\"].get(\"source\", \"?\") for c in raw_chunks)\n",
        "section_counts = Counter(\n",
        "    f\"{c['metadata'].get('source','?')}/{c['metadata'].get('section','?')}\"\n",
        "    for c in raw_chunks\n",
        ")\n",
        "\n",
        "print(f\"Total chunks: {len(raw_chunks)}\\n\")\n",
        "print(\"By source:\")\n",
        "for src, cnt in sorted(source_counts.items()):\n",
        "    print(f\"  {src:20s} {cnt:4d} chunks\")\n",
        "print(\"\\nBy source/section:\")\n",
        "for key, cnt in sorted(section_counts.items()):\n",
        "    print(f\"  {key:40s} {cnt:4d} chunks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Showing 10 of 18265 chunks (source=lp)\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "Chunk 0  |  source=lp  section=objective\n",
            "         |  chars=22271\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "Objective: Minimize x_p_ind[0,0] + x_p_ind[0,1] + x_p_ind[0,2] + x_p_ind[0,3] + x_p_ind[0,4] +\n",
            "  x_p_ind[0,5] + x_p_ind[0,6] + x_p_ind[0,7] + 2 x_p_ind[0,8] + 2 x_p_ind[0,9] + 2 x_p_ind[0,10] + 2\n",
            "  x_p_ind[0,11] + 2 x_p_ind[0,12] + 2 x_p_ind[0,13] + 2 x_p_ind[0,14] + 2 x_p_ind[0,15] + 2\n",
            "  x_p_ind[0,16] + 2 x_p_ind[0,17] + 2 x_p_ind[0,18] + 2 x_p_ind[0,19] + 2 x_p_ind[0,20] + 2\n",
            "  x_p_ind[0,21] + x_p_ind[1,0] + x_p_ind[1,1] + x_p_ind[1,2] + x_p_ind[1,3] + x_p_ind[1,4] +\n",
            "  x_p_ind[1,5] + x_p_ind[1,6] + x_p_ind[1,7] + 2 x_p_ind[1,8] + 2 x_p_ind[1,9] + 2 x_p_ind[1,10] + 2\n",
            "  x_p_ind[1,11] + 2 x_p_ind[1,12] + 2 x_p_ind[1,13] + 2 x_p_ind[1,14] + 2 x_p_ind[1,15] + 2\n",
            "  x_p_ind[1,16] + 2 x_p_ind[1,17] + 2 x_p_ind[1,18] + 2 x_p_ind[1,19] + 2 x_p_ind[1,20] + 2\n",
            "  x_p_ind[1,21] + x_p_ind[2,0] + x_p_ind[2,1] + x_p_ind[2,2] + x_p_ind[2,3] + x_p_ind[2,4] +\n",
            "  x_p_ind[2,5] + x_p_ind[2,6] + x_p_ind[2,7] + x_p_ind[2,8] + x_p_ind[2,9] + x_p_ind[2,10] +\n",
            "  x_p_ind[2,11] + x_p_ind[2,12] + x_p_ind[2,13] + 2 x_p_ind[2,14] + 2 x_p_in ... [truncated]\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "Chunk 1  |  source=lp  section=constraints\n",
            "         |  {'constraint_name': 'demand_balance_project_0_week_0'}\n",
            "         |  chars=78\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "Constraint: demand_balance_project_0_week_0 1.5 x[0,0,0] + x[1,0,0] + x[2,0,0]\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "Chunk 2  |  source=lp  section=constraints\n",
            "         |  {'constraint_name': 'demand_balance_project_1_week_0'}\n",
            "         |  chars=78\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "Constraint: demand_balance_project_1_week_0 1.5 x[0,0,1] + x[1,0,1] + x[2,0,1]\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "Chunk 3  |  source=lp  section=constraints\n",
            "         |  {'constraint_name': 'demand_balance_project_2_week_0'}\n",
            "         |  chars=78\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "Constraint: demand_balance_project_2_week_0 1.5 x[0,0,2] + x[1,0,2] + x[2,0,2]\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "Chunk 4  |  source=lp  section=constraints\n",
            "         |  {'constraint_name': 'demand_balance_project_3_week_0'}\n",
            "         |  chars=78\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "Constraint: demand_balance_project_3_week_0 1.5 x[0,0,3] + x[1,0,3] + x[2,0,3]\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "Chunk 5  |  source=lp  section=constraints\n",
            "         |  {'constraint_name': 'demand_balance_project_4_week_0'}\n",
            "         |  chars=78\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "Constraint: demand_balance_project_4_week_0 1.5 x[0,0,4] + x[1,0,4] + x[2,0,4]\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "Chunk 6  |  source=lp  section=constraints\n",
            "         |  {'constraint_name': 'demand_balance_project_5_week_0'}\n",
            "         |  chars=78\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "Constraint: demand_balance_project_5_week_0 1.5 x[0,0,5] + x[1,0,5] + x[2,0,5]\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "Chunk 7  |  source=lp  section=constraints\n",
            "         |  {'constraint_name': 'demand_balance_project_6_week_0'}\n",
            "         |  chars=78\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "Constraint: demand_balance_project_6_week_0 1.5 x[0,0,6] + x[1,0,6] + x[2,0,6]\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "Chunk 8  |  source=lp  section=constraints\n",
            "         |  {'constraint_name': 'demand_balance_project_7_week_0'}\n",
            "         |  chars=78\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "Constraint: demand_balance_project_7_week_0 1.5 x[0,0,7] + x[1,0,7] + x[2,0,7]\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "Chunk 9  |  source=lp  section=constraints\n",
            "         |  {'constraint_name': 'demand_balance_project_8_week_0'}\n",
            "         |  chars=78\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "Constraint: demand_balance_project_8_week_0 1.5 x[0,0,8] + x[1,0,8] + x[2,0,8]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# === Browse chunks: pick a source to sample ===\n",
        "# Change SOURCE_FILTER to inspect different sources: \"py\", \"lp\", \"mps\", \"index_mapping\", or None for all\n",
        "SOURCE_FILTER = \"lp\"       # <-- change me\n",
        "MAX_DISPLAY   = 10         # how many chunks to show\n",
        "\n",
        "filtered = [c for c in raw_chunks if SOURCE_FILTER is None or c[\"metadata\"].get(\"source\") == SOURCE_FILTER]\n",
        "print(f\"Showing {min(MAX_DISPLAY, len(filtered))} of {len(filtered)} chunks (source={SOURCE_FILTER or 'all'})\\n\")\n",
        "\n",
        "for i, chunk in enumerate(filtered[:MAX_DISPLAY]):\n",
        "    meta = chunk[\"metadata\"]\n",
        "    text = chunk[\"text\"]\n",
        "    # Header\n",
        "    print(f\"{'─' * 80}\")\n",
        "    print(f\"Chunk {i}  |  source={meta.get('source')}  section={meta.get('section')}\")\n",
        "    extra_keys = {k: v for k, v in meta.items() if k not in (\"source\", \"section\", \"path\")}\n",
        "    if extra_keys:\n",
        "        print(f\"         |  {extra_keys}\")\n",
        "    print(f\"         |  chars={len(text)}\")\n",
        "    print(f\"{'─' * 80}\")\n",
        "    # Word-wrapped text (first 600 chars if very long)\n",
        "    display_text = text if len(text) <= 1000 else text[:1000] + \"\\n... [truncated]\"\n",
        "    print(textwrap.fill(display_text, width=100, subsequent_indent=\"  \"))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3b. Inspect Persisted Index: Docstore Nodes & Embedding Vectors\n",
        "\n",
        "After LlamaIndex builds the index, it sub-chunks your documents into smaller nodes and embeds each one.\n",
        "The persisted files (`docstore.json`, `default__vector_store.json`) are single-line JSON and too large to open in an IDE.\n",
        "This cell **samples** a few nodes and their embeddings so you can inspect them here.\n",
        "\n",
        "> **Note**: `docstore.json` (31 MB, 18k+ nodes) and `default__vector_store.json` (646 MB, 18k embeddings × 1536 dims) are written by LlamaIndex in compact single-line JSON. The cell below loads and pretty-prints samples without modifying the files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Docstore: 18533 nodes total\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────────────────────────\n",
            "  Node 0  id=644641d6-dac...  source=lp  section=constraints\n",
            "  chars=81  metadata_keys=['source', 'section', 'constraint_name', 'path']\n",
            "──────────────────────────────────────────────────────────────────────────────────────────\n",
            "  Constraint: indicator_constraint_0_10_23_13 - 1e+08 x[10,23,13] + x_ind[10,23,13]\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────────────────────────\n",
            "  Node 1  id=d64b7830-db0...  source=lp  section=constraints\n",
            "  chars=83  metadata_keys=['source', 'section', 'constraint_name', 'path']\n",
            "──────────────────────────────────────────────────────────────────────────────────────────\n",
            "  Constraint: indicator_constraint_0_5_20_10 - 1e+08 x[5,20,10] + x_ind[5,20,10] <= 0\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────────────────────────\n",
            "  Node 2  id=b1d41113-c99...  source=lp  section=constraints\n",
            "  chars=80  metadata_keys=['source', 'section', 'constraint_name', 'path']\n",
            "──────────────────────────────────────────────────────────────────────────────────────────\n",
            "  Constraint: indicator_constraint_1_11_19_12 - x[11,19,12] + x_ind[11,19,12] >= 0\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────────────────────────\n",
            "  Node 3  id=e5e62acb-fa4...  source=lp  section=constraints\n",
            "  chars=81  metadata_keys=['source', 'section', 'constraint_name', 'path']\n",
            "──────────────────────────────────────────────────────────────────────────────────────────\n",
            "  Constraint: indicator_constraint_0_12_20_10 - 1e+08 x[12,20,10] + x_ind[12,20,10]\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────────────────────────\n",
            "  Node 4  id=2b6f6e22-407...  source=lp  section=constraints\n",
            "  chars=77  metadata_keys=['source', 'section', 'constraint_name', 'path']\n",
            "──────────────────────────────────────────────────────────────────────────────────────────\n",
            "  Constraint: indicator_constraint_0_6_1_7 - 1e+08 x[6,1,7] + x_ind[6,1,7] <= 0\n",
            "\n",
            "==========================================================================================\n",
            "  Sampling from default__vector_store.json (embeddings)\n",
            "==========================================================================================\n",
            "\n",
            "Total embeddings: 18533\n",
            "Embedding dimension: 1536\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────────────────────────\n",
            "  Embedding 0  id=2c0c03b1-ddf...\n",
            "  ref_doc_id=0245b93e-ef4...  metadata={'source': 'lp', 'section': 'constraints', 'constraint_name': 'indicator_constraint_0_6_15_13', 'path': '/Users/Larry.Jin/Documents/research/agent_explain/outputs/model.lp', '_node_type': 'TextNode', 'document_id': '0245b93e-ef4c-4cdd-955e-47895095fb2f', 'doc_id': '0245b93e-ef4c-4cdd-955e-47895095fb2f', 'ref_doc_id': '0245b93e-ef4c-4cdd-955e-47895095fb2f'}\n",
            "  vector (first 8 dims): [0.010579, 0.004188, -0.001106, -0.011296, 0.011829, 0.023232, -0.029896, 0.007481]...\n",
            "  vector (last  4 dims): ...[-0.00132, -0.001851, -0.002677, -0.03143]\n",
            "  text: Constraint: indicator_constraint_0_6_15_13\n",
            "- 1e+08 x[6,15,13] + x_ind[6,15,13] <= 0\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────────────────────────\n",
            "  Embedding 1  id=fc1381fd-d59...\n",
            "  ref_doc_id=3809fbd9-45d...  metadata={'source': 'lp', 'section': 'constraints', 'constraint_name': 'indicator_constraint_0_0_17_14', 'path': '/Users/Larry.Jin/Documents/research/agent_explain/outputs/model.lp', '_node_type': 'TextNode', 'document_id': '3809fbd9-45d5-44d0-bbaf-80a1f34b1147', 'doc_id': '3809fbd9-45d5-44d0-bbaf-80a1f34b1147', 'ref_doc_id': '3809fbd9-45d5-44d0-bbaf-80a1f34b1147'}\n",
            "  vector (first 8 dims): [0.009952, 0.00509, -0.001027, -0.00581, 0.007272, 0.020888, -0.030826, 0.009474]...\n",
            "  vector (last  4 dims): ...[-0.002766, 0.000776, -0.003932, -0.03017]\n",
            "  text: Constraint: indicator_constraint_0_0_17_14\n",
            "- 1e+08 x[0,17,14] + x_ind[0,17,14] <= 0\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────────────────────────\n",
            "  Embedding 2  id=c3ef770f-ec7...\n",
            "  ref_doc_id=f02e84e7-40c...  metadata={'source': 'lp', 'section': 'constraints', 'constraint_name': 'indicator_constraint_0_7_9_17', 'path': '/Users/Larry.Jin/Documents/research/agent_explain/outputs/model.lp', '_node_type': 'TextNode', 'document_id': 'f02e84e7-40cb-4ef4-9304-60b370b521b5', 'doc_id': 'f02e84e7-40cb-4ef4-9304-60b370b521b5', 'ref_doc_id': 'f02e84e7-40cb-4ef4-9304-60b370b521b5'}\n",
            "  vector (first 8 dims): [0.00968, 0.005188, -0.001537, -0.005601, 0.008592, 0.021423, -0.030356, 0.007731]...\n",
            "  vector (last  4 dims): ...[-0.003748, -0.00271, -0.002598, -0.032091]\n",
            "  text: Constraint: indicator_constraint_0_7_9_17\n",
            "- 1e+08 x[7,9,17] + x_ind[7,9,17] <= 0\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────────────────────────\n",
            "  Embedding 3  id=0b54901c-0ae...\n",
            "  ref_doc_id=a4027645-9bd...  metadata={'source': 'lp', 'section': 'constraints', 'constraint_name': 'demand_balance_project_17_week_10', 'path': '/Users/Larry.Jin/Documents/research/agent_explain/outputs/model.lp', '_node_type': 'TextNode', 'document_id': 'a4027645-9bd0-4029-b86a-b51b2e1b6ede', 'doc_id': 'a4027645-9bd0-4029-b86a-b51b2e1b6ede', 'ref_doc_id': 'a4027645-9bd0-4029-b86a-b51b2e1b6ede'}\n",
            "  vector (first 8 dims): [0.002797, -0.005668, 0.004475, -0.011123, 0.008907, 0.036424, -0.0407, 0.020307]...\n",
            "  vector (last  4 dims): ...[-0.002977, -0.019808, -0.002312, -0.041042]\n",
            "  text: Constraint: demand_balance_project_17_week_10\n",
            "1.5 x[0,10,17] + x[1,10,17]\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────────────────────────\n",
            "  Embedding 4  id=21e9f94d-5a5...\n",
            "  ref_doc_id=a2e75506-1c3...  metadata={'source': 'lp', 'section': 'constraints', 'constraint_name': 'indicator_constraint_0_11_23_10', 'path': '/Users/Larry.Jin/Documents/research/agent_explain/outputs/model.lp', '_node_type': 'TextNode', 'document_id': 'a2e75506-1c35-4c6b-a9ad-dbdcd06a6e71', 'doc_id': 'a2e75506-1c35-4c6b-a9ad-dbdcd06a6e71', 'ref_doc_id': 'a2e75506-1c35-4c6b-a9ad-dbdcd06a6e71'}\n",
            "  vector (first 8 dims): [0.006116, 0.011961, -0.001789, -0.012025, 0.015479, 0.019263, -0.034599, 0.010685]...\n",
            "  vector (last  4 dims): ...[0.006962, -0.00994, -0.013852, -0.034226]\n",
            "  text: Constraint: indicator_constraint_0_11_23_10\n",
            "- 1e+08 x[11,23,10] + x_ind[11,23,10]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import json, textwrap, random\n",
        "\n",
        "rag_dir = OUTPUTS_DIR / \"rag_index\"\n",
        "NUM_SAMPLES = 5  # how many nodes/vectors to show\n",
        "\n",
        "# ── 1. Docstore: sample text nodes ──────────────────────────────────────────\n",
        "with open(rag_dir / \"docstore.json\", \"r\") as f:\n",
        "    docstore = json.load(f)\n",
        "\n",
        "doc_data = docstore.get(\"docstore/data\", {})\n",
        "doc_ids = list(doc_data.keys())\n",
        "print(f\"Docstore: {len(doc_ids)} nodes total\\n\")\n",
        "\n",
        "sample_ids = random.sample(doc_ids, min(NUM_SAMPLES, len(doc_ids)))\n",
        "for i, nid in enumerate(sample_ids):\n",
        "    entry = doc_data[nid]\n",
        "    d = entry.get(\"__data__\", entry)\n",
        "    text = d.get(\"text\", \"\")\n",
        "    meta = d.get(\"metadata\", {})\n",
        "    source = meta.get(\"source\", \"?\")\n",
        "    section = meta.get(\"section\", \"?\")\n",
        "    print(f\"{'─' * 90}\")\n",
        "    print(f\"  Node {i}  id={nid[:12]}...  source={source}  section={section}\")\n",
        "    print(f\"  chars={len(text)}  metadata_keys={list(meta.keys())}\")\n",
        "    print(f\"{'─' * 90}\")\n",
        "    display = text if len(text) <= 400 else text[:400] + \"\\n  ... [truncated]\"\n",
        "    print(textwrap.fill(display, width=95, initial_indent=\"  \", subsequent_indent=\"  \"))\n",
        "    print()\n",
        "\n",
        "# ── 2. Vector store: sample embeddings ──────────────────────────────────────\n",
        "# Stream-parse to avoid loading 646MB into memory all at once\n",
        "# We just need a few sample keys from embedding_dict\n",
        "print(f\"{'=' * 90}\")\n",
        "print(\"  Sampling from default__vector_store.json (embeddings)\")\n",
        "print(f\"{'=' * 90}\\n\")\n",
        "\n",
        "# Load only the metadata and text_id mapping (small), and sample embedding keys\n",
        "vs_path = rag_dir / \"default__vector_store.json\"\n",
        "# Read the full file — it's large but we only extract what we need\n",
        "with open(vs_path, \"r\") as f:\n",
        "    vs_data = json.load(f)\n",
        "\n",
        "emb_dict = vs_data.get(\"embedding_dict\", {})\n",
        "text_to_doc = vs_data.get(\"text_id_to_ref_doc_id\", {})\n",
        "meta_dict = vs_data.get(\"metadata_dict\", {})\n",
        "emb_ids = list(emb_dict.keys())\n",
        "print(f\"Total embeddings: {len(emb_ids)}\")\n",
        "if emb_ids:\n",
        "    dim = len(emb_dict[emb_ids[0]])\n",
        "    print(f\"Embedding dimension: {dim}\\n\")\n",
        "\n",
        "sample_emb_ids = random.sample(emb_ids, min(NUM_SAMPLES, len(emb_ids)))\n",
        "for i, eid in enumerate(sample_emb_ids):\n",
        "    vec = emb_dict[eid]\n",
        "    ref_doc = text_to_doc.get(eid, \"?\")\n",
        "    meta = meta_dict.get(eid, {})\n",
        "    # Look up the text from docstore\n",
        "    doc_entry = doc_data.get(eid, {})\n",
        "    doc_d = doc_entry.get(\"__data__\", doc_entry) if doc_entry else {}\n",
        "    node_text = doc_d.get(\"text\", \"(not in docstore)\")\n",
        "\n",
        "    print(f\"{'─' * 90}\")\n",
        "    print(f\"  Embedding {i}  id={eid[:12]}...\")\n",
        "    print(f\"  ref_doc_id={ref_doc[:12]}...  metadata={meta}\")\n",
        "    print(f\"  vector (first 8 dims): {[round(v, 6) for v in vec[:8]]}...\")\n",
        "    print(f\"  vector (last  4 dims): ...{[round(v, 6) for v in vec[-4:]]}\")\n",
        "    text_preview = node_text if len(node_text) <= 200 else node_text[:200] + \"...\"\n",
        "    print(f\"  text: {text_preview}\")\n",
        "    print()\n",
        "\n",
        "del vs_data, emb_dict  # free memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Run the workflow\n",
        "\n",
        "Load baseline and RAG index, then run the agentic workflow for a natural-language query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "from use_case.staffing_model import build_gurobi_model\n",
        "from agentic_explain.rag.plain_rag import build_plain_rag\n",
        "from agentic_explain.workflow.graph import create_workflow, invoke_workflow\n",
        "\n",
        "# Load baseline and build Plain RAG strategy\n",
        "with open(OUTPUTS_DIR / \"baseline_result.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    baseline_result = json.load(f)\n",
        "\n",
        "rag_strategy = build_plain_rag(\n",
        "    py_path=PROJECT_ROOT / \"use_case\" / \"staffing_model\" / \"staffing_model.py\",\n",
        "    lp_path=OUTPUTS_DIR / \"model.lp\",\n",
        "    mps_path=OUTPUTS_DIR / \"model.mps\",\n",
        "    data_dir=DATA_DIR,\n",
        "    persist_dir=OUTPUTS_DIR / \"rag_index\",\n",
        ")\n",
        "openai_client = OpenAI()\n",
        "\n",
        "workflow = create_workflow(\n",
        "    openai_client=openai_client,\n",
        "    rag_strategy=rag_strategy,\n",
        "    baseline_result=baseline_result,\n",
        "    data_dir=str(DATA_DIR),\n",
        "    build_model_fn=build_gurobi_model,\n",
        "    inputs=inputs,\n",
        "    env_kwargs=env_kwargs,\n",
        "    outputs_dir=str(OUTPUTS_DIR),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Set parameter WLSAccessID\n",
            "Set parameter WLSSecret\n",
            "Set parameter LicenseID to value 2678051\n",
            "WLS license 2678051 - registered to C3.ai\n",
            "Set parameter TimeLimit to value 100\n",
            "Gurobi Optimizer version 13.0.1 build v13.0.1rc0 (mac64[arm] - Darwin 25.2.0 25C56)\n",
            "\n",
            "CPU model: Apple M3 Max\n",
            "Thread count: 16 physical cores, 16 logical processors, using up to 16 threads\n",
            "\n",
            "Non-default parameters:\n",
            "TimeLimit  100\n",
            "\n",
            "WLS license 2678051 - registered to C3.ai\n",
            "Optimize a model with 18263 rows, 17260 columns and 83921 nonzeros (Min)\n",
            "Model fingerprint: 0xf012b5d7\n",
            "Model has 1244 linear objective coefficients\n",
            "Variable types: 8944 continuous, 8316 integer (8316 binary)\n",
            "Coefficient statistics:\n",
            "  Matrix range     [1e+00, 1e+08]\n",
            "  Objective range  [6e-01, 2e+00]\n",
            "  Bounds range     [1e+00, 1e+00]\n",
            "  RHS range        [5e-01, 6e+00]\n",
            "\n",
            "Found heuristic solution: objective 782.1263000\n",
            "Presolve removed 6447 rows and 6088 columns\n",
            "Presolve time: 0.03s\n",
            "Presolved: 11816 rows, 11172 columns, 47071 nonzeros\n",
            "Variable types: 5783 continuous, 5389 integer (5389 binary)\n",
            "\n",
            "Root relaxation: objective 1.073864e+02, 24845 iterations, 1.22 seconds (2.13 work units)\n",
            "\n",
            "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
            " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
            "\n",
            "     0     0  107.38643    0  155  782.12630  107.38643  86.3%     -    1s\n",
            "H    0     0                     185.4931667  107.38643  42.1%     -    1s\n",
            "H    0     0                     181.4390001  107.38643  40.8%     -    1s\n",
            "H    0     0                     178.9250000  107.38643  40.0%     -    1s\n",
            "H    0     0                     176.7220000  107.71108  39.1%     -    3s\n",
            "H    0     0                     165.9265000  107.71108  35.1%     -    3s\n",
            "H    0     0                     156.6718333  107.71108  31.3%     -    3s\n",
            "     0     0  107.71108    0  718  156.67183  107.71108  31.3%     -    3s\n",
            "H    0     0                     156.6653334  107.83254  31.2%     -    4s\n",
            "     0     0  108.35137    0  810  156.66533  108.35137  30.8%     -    4s\n",
            "H    0     0                     153.3723332  108.63091  29.2%     -    7s\n",
            "H    0     0                     151.8690001  108.63091  28.5%     -    7s\n",
            "     0     0  108.63091    0 1156  151.86900  108.63091  28.5%     -    7s\n",
            "H    0     0                     143.2466661  109.00932  23.9%     -    9s\n",
            "H    0     0                     142.4228334  109.00932  23.5%     -    9s\n",
            "H    0     0                     142.4228333  109.00932  23.5%     -    9s\n",
            "     0     0  110.16310    0 1189  142.42283  110.16310  22.7%     -    9s\n",
            "H    0     0                     140.5895000  110.16310  21.6%     -   10s\n",
            "     0     0  110.16310    0 1375  140.58950  110.16310  21.6%     -   10s\n",
            "     0     0  110.16310    0 1420  140.58950  110.16310  21.6%     -   11s\n",
            "     0     0  110.16310    0 1496  140.58950  110.16310  21.6%     -   12s\n",
            "     0     0  110.16310    0 1464  140.58950  110.16310  21.6%     -   12s\n",
            "     0     0  110.16310    0 1466  140.58950  110.16310  21.6%     -   12s\n",
            "     0     0  110.16310    0 1434  140.58950  110.16310  21.6%     -   12s\n",
            "     0     0  110.35584    0 1447  140.58950  110.35584  21.5%     -   12s\n",
            "     0     0  110.35584    0 1439  140.58950  110.35584  21.5%     -   13s\n",
            "     0     0  110.79768    0 1671  140.58950  110.79768  21.2%     -   18s\n",
            "H    0     0                     137.5151666  111.46562  18.9%     -   23s\n",
            "H    0     0                     132.3115000  111.46562  15.8%     -   23s\n",
            "     0     0  111.50914    0 1728  132.31150  111.50914  15.7%     -   23s\n",
            "H    0     0                     131.6035001  111.73758  15.1%     -   27s\n",
            "     0     0  111.73758    0 1848  131.60350  111.73758  15.1%     -   27s\n",
            "     0     0  111.88689    0 1857  131.60350  111.88689  15.0%     -   30s\n",
            "     0     0  111.94606    0 1866  131.60350  111.94606  14.9%     -   31s\n",
            "     0     0  111.99040    0 1962  131.60350  111.99040  14.9%     -   33s\n",
            "     0     0  112.00206    0 1925  131.60350  112.00206  14.9%     -   33s\n",
            "     0     0  112.00746    0 1929  131.60350  112.00746  14.9%     -   34s\n",
            "     0     0  112.00834    0 1950  131.60350  112.00834  14.9%     -   34s\n",
            "H    0     0                     131.6035000  112.87910  14.2%     -   45s\n",
            "H    0     0                     131.6035000  112.87910  14.2%     -   45s\n",
            "H    0     0                     131.6034999  112.87910  14.2%     -   45s\n",
            "     0     0  113.23977    0 1984  131.60350  113.23977  14.0%     -   45s\n",
            "H    0     0                     131.5860000  113.40889  13.8%     -   52s\n",
            "H    0     0                     131.5766665  113.40889  13.8%     -   52s\n",
            "H    0     0                     131.0075000  113.40889  13.4%     -   52s\n",
            "H    0     0                     130.7150000  113.40889  13.2%     -   52s\n",
            "H    0     0                     130.7150000  113.40889  13.2%     -   52s\n",
            "H    0     0                     130.7150000  113.40889  13.2%     -   52s\n",
            "H    0     0                     130.7149998  113.40889  13.2%     -   52s\n",
            "     0     0  113.51825    0 2131  130.71500  113.51825  13.2%     -   52s\n",
            "H    0     0                     130.6779999  113.65271  13.0%     -   58s\n",
            "     0     0  113.65271    0 2113  130.67800  113.65271  13.0%     -   58s\n",
            "H    0     0                     130.6779999  113.95308  12.8%     -   64s\n",
            "H    0     0                     130.3250000  113.95308  12.6%     -   64s\n",
            "     0     0  113.95308    0 2213  130.32500  113.95308  12.6%     -   64s\n",
            "H    0     0                     129.9690000  114.09774  12.2%     -   68s\n",
            "     0     0  114.09774    0 2077  129.96900  114.09774  12.2%     -   68s\n",
            "     0     0  114.24022    0 2203  129.96900  114.24022  12.1%     -   71s\n",
            "H    0     0                     129.9689999  114.24022  12.1%     -   74s\n",
            "H    0     0                     129.6145000  114.24022  11.9%     -   74s\n",
            "H    0     0                     129.6144997  114.24022  11.9%     -   74s\n",
            "     0     0  114.37162    0 2076  129.61450  114.37162  11.8%     -   74s\n",
            "H    0     0                     127.6084998  114.37162  10.4%     -   75s\n",
            "H    0     0                     127.5209998  114.37162  10.3%     -   75s\n",
            "     0     0  114.37162    0 2096  127.52100  114.37162  10.3%     -   75s\n",
            "     0     0  114.37162    0 2136  127.52100  114.37162  10.3%     -   76s\n",
            "H    0     0                     127.4150000  114.37162  10.2%     -   78s\n",
            "     0     0  114.37162    0 2144  127.41500  114.37162  10.2%     -   78s\n",
            "     0     0  114.64414    0 2041  127.41500  114.64414  10.0%     -   78s\n",
            "     0     0  114.64414    0 2074  127.41500  114.64414  10.0%     -   79s\n",
            "H    0     0                     126.5161666  114.64414  9.38%     -   85s\n",
            "H    0     0                     126.4576667  114.90230  9.14%     -   95s\n",
            "H    0     0                     126.4576666  114.90230  9.14%     -   95s\n",
            "H    0     0                     126.4336665  114.90230  9.12%     -   95s\n",
            "     0     0  115.05736    0 1991  126.43367  115.05736  9.00%     -   95s\n",
            "     0     0          -    0       126.43367  115.05736  9.00%     -  100s\n",
            "\n",
            "Cutting planes:\n",
            "  Gomory: 2\n",
            "  Lift-and-project: 1\n",
            "  Cover: 3\n",
            "  Implied bound: 1803\n",
            "  Clique: 869\n",
            "  MIR: 524\n",
            "  Flow cover: 1628\n",
            "  RLT: 38\n",
            "  Relax-and-lift: 60\n",
            "  BQP: 8\n",
            "  PSD: 54\n",
            "\n",
            "Explored 1 nodes (255039 simplex iterations) in 100.02 seconds (177.04 work units)\n",
            "Thread count was 16 (of 16 available processors)\n",
            "\n",
            "Solution count 10: 126.434 126.458 126.458 ... 129.969\n",
            "\n",
            "Time limit reached\n",
            "Best objective 1.264336664604e+02, best bound 1.150573627158e+02, gap 8.9978%\n"
          ]
        }
      ],
      "source": [
        "# Example query: counterfactual (\"why not\")\n",
        "user_query = \"Why was Josh not staffed on Ipp IO Pilot in week 6?\"\n",
        "\n",
        "final_state = invoke_workflow(\n",
        "    workflow,\n",
        "    user_query,\n",
        "    baseline_result=baseline_result,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: Why was Josh not staffed on Ipp IO Pilot in week 6?\n",
            "\n",
            "Summary:\n",
            "The user's change resulted in a total objective improvement of approximately\n",
            "**$3.37** (a 2.6% decrease in total objective value). Key drivers of this change\n",
            "included a slight reduction in the cost of missing demand and a notable decrease\n",
            "in staffing consistency (down 11.6%), indicating increased context switching\n",
            "among employees.   While there was a minor improvement in unmet staffing demand,\n",
            "this was offset by an increase in idle time by 2.2% and a significant rise in\n",
            "the out-of-cohort penalty by 50%, suggesting that while some resources were\n",
            "utilized more effectively, staffing decisions led to misalignments with\n",
            "employees' preferred projects, negatively impacting overall efficiency.\n"
          ]
        }
      ],
      "source": [
        "print(\"Query:\", user_query)\n",
        "print()\n",
        "print(\"Summary:\")\n",
        "import textwrap\n",
        "print(textwrap.fill(\n",
        "    final_state.get(\"final_summary\", \"(no summary)\"), \n",
        "    width=80\n",
        "    ))\n",
        "\n",
        "# Optional: print full debug (retrieval, LLM messages, applied constraints, comparison)\n",
        "# from agentic_explain.workflow import debug\n",
        "# debug.print_workflow_summary(final_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Evaluation Dataset\n",
        "\n",
        "Load the evaluation query dataset (`agentic_explain/evaluation/queries.json`) and run individual queries.\n",
        "Each query has a **reference answer** for LLM-as-judge evaluation.\n",
        "\n",
        "| Category | Count | Description |\n",
        "|----------|-------|-------------|\n",
        "| `objective / missing_demand` | 7 | Why is project X understaffed in week Y? |\n",
        "| `objective / idle_time` | 3 | Why is employee X idle in week Y? |\n",
        "| `objective / staffing_consistency` | 2 | Why is employee X on project Y? |\n",
        "| `objective / out_of_cohort_penalty` | 2 | Why is employee X (cohort A) on project Y (cohort B)? |\n",
        "| `constraint / max_concurrency` | 5 | Why is employee X not on project Y? (at concurrency limit) |\n",
        "| `constraint / demand_balance_inactive` | 4 | Why is employee X not on project Y in week Z? (project inactive) |\n",
        "| `constraint / oversight_requirement` | 2 | Oversight-related staffing questions |\n",
        "| `constraint / employee_allocation` | 1 | Capacity (100%) constraint |\n",
        "| `mixed` | 3 | Peak crunch, specific employee requirements |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 29 evaluation queries\n",
            "\n",
            "  [ 0] [F] objective  / missing_demand                Why is IO Base development understaffed in week 10?\n",
            "  [ 1] [F] objective  / missing_demand                Why is PSO Base development missing demand in week 15?\n",
            "  [ 2] [F] objective  / missing_demand                Why is Saa DF Pilot not fully staffed in week 14?\n",
            "  [ 3] [F] objective  / missing_demand                Why is DF Base development understaffed in week 16?\n",
            "  [ 4] [F] objective  / missing_demand                Why is Foo PSO Pilot understaffed in week 11?\n",
            "  [ 5] [F] objective  / missing_demand                Why is Saa PSO Pilot not fully staffed in week 19?\n",
            "  [ 6] [F] objective  / missing_demand                Why does IO Base development have unmet demand in week 20?\n",
            "  [ 7] [F] objective  / idle_time                     Why is Yimin idle in week 0?\n",
            "  [ 8] [F] objective  / idle_time                     Why is Sruti idle in week 3?\n",
            "  [ 9] [F] objective  / idle_time                     Why is Yimin idle in week 24?\n",
            "  [10] [F] objective  / staffing_consistency          Why is Larry assigned to Paa PSO COE in week 5?\n",
            "  [11] [F] objective  / staffing_consistency          Why is Jianchen working on Cmm IO Pilot in week 8?\n",
            "  [12] [F] objective  / out_of_cohort_penalty         Why is Yimin assigned to Koo DF Pilot in week 15?\n",
            "  [13] [F] objective  / out_of_cohort_penalty         Why is Bhavya working on IO Base development in week 4?\n",
            "  [14] [F] constraint / max_concurrency               Why is Utsav not staffed on PSO Base development in week 5?\n",
            "  [15] [F] constraint / max_concurrency               Why is Minnie not working on IO product demo in week 3?\n",
            "  [16] [F] constraint / max_concurrency               Why is Shivarjun not on IO Base development in week 10?\n",
            "  [17] [F] constraint / max_concurrency               Why is Arpit not on Hee DF COE in week 5?\n",
            "  [18] [F] constraint / max_concurrency               Why is Larry not also working on Saa PSO Pilot in week 12?\n",
            "  [19] [I] constraint / demand_balance_inactive       Why is Josh not on Foo PSO Pilot in week 5?\n",
            "  [20] [I] constraint / demand_balance_inactive       Why is Nancy not working on Saa PSO Pilot in week 8?\n",
            "  [21] [I] constraint / demand_balance_inactive       Why is Sruti not working on DF product demo in week 10?\n",
            "  [22] [I] constraint / demand_balance_inactive       Why is Yimin not working on Moo IO Pilot in week 5?\n",
            "  [23] [F] constraint / oversight_requirement         Why is Josh not allocated more to PSO Base development in week 12?\n",
            "  [24] [F] constraint / oversight_requirement         Why is Larry the one providing oversight on Foo PSO Pilot instead of Josh?\n",
            "  [25] [F] constraint / employee_allocation           Why is Shivarjun not helping with IO Base development in week 15?\n",
            "  [26] [F] mixed      / peak_crunch                   Why is Stefano not staffed on Foo PSO Pilot in week 12?\n",
            "  [27] [F] mixed      / peak_crunch                   Why is Jason not staffed on Nuu DF Pilot in week 14?\n",
            "  [28] [F] constraint / specific_employee_requirement  Why is Nancy not working on PSO v8 migration in week 3 instead of Foo PSO Pilot?\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "eval_path = PROJECT_ROOT / \"agentic_explain\" / \"evaluation\" / \"queries.json\"\n",
        "with open(eval_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    eval_queries = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(eval_queries)} evaluation queries\\n\")\n",
        "\n",
        "# Preview all queries\n",
        "for i, q in enumerate(eval_queries):\n",
        "    path_marker = \"F\" if q[\"expected_path\"] == \"feasible\" else \"I\"\n",
        "    print(f\"  [{i:2d}] [{path_marker}] {q['category']:10s} / {q['subcategory']:28s}  {q['query']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[obj2_idle_time_01]  objective / idle_time\n",
            "Query:    Why is Yimin idle in week 0?\n",
            "Expected: path=feasible, expr=x_idle[5,0] == 0\n",
            "================================================================================\n",
            "Set parameter WLSAccessID\n",
            "Set parameter WLSSecret\n",
            "Set parameter LicenseID to value 2678051\n",
            "WLS license 2678051 - registered to C3.ai\n",
            "Set parameter TimeLimit to value 100\n",
            "Gurobi Optimizer version 13.0.1 build v13.0.1rc0 (mac64[arm] - Darwin 25.2.0 25C56)\n",
            "\n",
            "CPU model: Apple M3 Max\n",
            "Thread count: 16 physical cores, 16 logical processors, using up to 16 threads\n",
            "\n",
            "Non-default parameters:\n",
            "TimeLimit  100\n",
            "\n",
            "WLS license 2678051 - registered to C3.ai\n",
            "Optimize a model with 18263 rows, 17260 columns and 83921 nonzeros (Min)\n",
            "Model fingerprint: 0x32c52b2e\n",
            "Model has 1244 linear objective coefficients\n",
            "Variable types: 8944 continuous, 8316 integer (8316 binary)\n",
            "Coefficient statistics:\n",
            "  Matrix range     [1e+00, 1e+08]\n",
            "  Objective range  [6e-01, 2e+00]\n",
            "  Bounds range     [1e+00, 1e+00]\n",
            "  RHS range        [5e-01, 6e+00]\n",
            "\n",
            "Found heuristic solution: objective 781.8475333\n",
            "Presolve removed 6419 rows and 6063 columns\n",
            "Presolve time: 0.03s\n",
            "Presolved: 11844 rows, 11197 columns, 47227 nonzeros\n",
            "Variable types: 5795 continuous, 5402 integer (5402 binary)\n",
            "\n",
            "Root relaxation: objective 1.063575e+02, 23341 iterations, 1.07 seconds (1.84 work units)\n",
            "\n",
            "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
            " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
            "\n",
            "     0     0  106.35750    0  138  781.84753  106.35750  86.4%     -    1s\n",
            "H    0     0                     671.3373667  106.35750  84.2%     -    1s\n",
            "H    0     0                     528.3090000  106.35750  79.9%     -    1s\n",
            "H    0     0                     305.0453000  106.35750  65.1%     -    1s\n",
            "H    0     0                     186.2515000  106.35750  42.9%     -    1s\n",
            "     0     0  106.60845    0  845  186.25150  106.60845  42.8%     -    2s\n",
            "     0     0  106.75644    0  810  186.25150  106.75644  42.7%     -    3s\n",
            "H    0     0                     169.6708331  106.75644  37.1%     -    3s\n",
            "H    0     0                     157.2001666  106.75644  32.1%     -    3s\n",
            "H    0     0                     156.7816665  106.75644  31.9%     -    3s\n",
            "H    0     0                     149.4895001  107.43456  28.1%     -    7s\n",
            "H    0     0                     149.4895000  107.43456  28.1%     -    7s\n",
            "H    0     0                     147.2411667  107.43456  27.0%     -    7s\n",
            "H    0     0                     146.6871667  107.43456  26.8%     -    7s\n",
            "     0     0  107.43456    0 1303  146.68717  107.43456  26.8%     -    7s\n",
            "     0     0  107.79856    0 1344  146.68717  107.79856  26.5%     -   10s\n",
            "     0     0  108.70508    0 1453  146.68717  108.70508  25.9%     -   11s\n",
            "     0     0  108.70508    0 1373  146.68717  108.70508  25.9%     -   12s\n",
            "     0     0  108.70508    0 1459  146.68717  108.70508  25.9%     -   13s\n",
            "     0     0  108.70508    0 1459  146.68717  108.70508  25.9%     -   13s\n",
            "H    0     0                     146.6196666  108.70508  25.9%     -   14s\n",
            "     0     0  108.70508    0 1452  146.61967  108.70508  25.9%     -   14s\n",
            "     0     0  108.70508    0 1480  146.61967  108.70508  25.9%     -   14s\n",
            "     0     0  108.70508    0 1504  146.61967  108.70508  25.9%     -   14s\n",
            "     0     0  108.70508    0 1501  146.61967  108.70508  25.9%     -   14s\n",
            "     0     0  109.87300    0 1786  146.61967  109.87300  25.1%     -   24s\n",
            "H    0     0                     144.8863333  110.43445  23.8%     -   31s\n",
            "H    0     0                     143.1093331  110.43445  22.8%     -   31s\n",
            "     0     0  110.43445    0 1903  143.10933  110.43445  22.8%     -   31s\n",
            "     0     0  110.60201    0 1912  143.10933  110.60201  22.7%     -   34s\n",
            "     0     0  110.67004    0 1977  143.10933  110.67004  22.7%     -   37s\n",
            "H    0     0                     143.0783333  110.70361  22.6%     -   39s\n",
            "     0     0  110.70361    0 1930  143.07833  110.70361  22.6%     -   39s\n",
            "     0     0  110.72887    0 1994  143.07833  110.72887  22.6%     -   42s\n",
            "     0     0  110.73550    0 2010  143.07833  110.73550  22.6%     -   43s\n",
            "     0     0  110.74000    0 1975  143.07833  110.74000  22.6%     -   44s\n",
            "     0     0  110.74111    0 2008  143.07833  110.74111  22.6%     -   44s\n",
            "H    0     0                     143.0303334  111.67381  21.9%     -   56s\n",
            "H    0     0                     139.2053334  111.67381  19.8%     -   56s\n",
            "     0     0  111.67381    0 2124  139.20533  111.67381  19.8%     -   56s\n",
            "H    0     0                     139.2053333  111.67381  19.8%     -   56s\n",
            "H    0     0                     138.7821666  111.67381  19.5%     -   56s\n",
            "H    0     0                     138.2029999  111.67381  19.2%     -   56s\n",
            "     0     0  112.01044    0 2217  138.20300  112.01044  19.0%     -   63s\n",
            "H    0     0                     134.9413331  112.11480  16.9%     -   66s\n",
            "     0     0  112.11480    0 2231  134.94133  112.11480  16.9%     -   66s\n",
            "     0     0  112.17946    0 2225  134.94133  112.17946  16.9%     -   69s\n",
            "H    0     0                     134.9138333  112.20842  16.8%     -   71s\n",
            "H    0     0                     134.9138330  112.20842  16.8%     -   71s\n",
            "     0     0  112.20842    0 2283  134.91383  112.20842  16.8%     -   71s\n",
            "H    0     0                     134.2853332  112.22860  16.4%     -   73s\n",
            "     0     0  112.22860    0 2267  134.28533  112.22860  16.4%     -   73s\n",
            "     0     0  112.23614    0 2294  134.28533  112.23614  16.4%     -   74s\n",
            "     0     0  112.23801    0 2296  134.28533  112.23801  16.4%     -   75s\n",
            "H    0     0                     134.2738334  112.23801  16.4%     -   83s\n",
            "     0     0  113.23697    0 1612  134.27383  113.23697  15.7%     -   93s\n",
            "H    0     0                     133.8583333  113.49100  15.2%     -  102s\n",
            "     0     0  113.49100    0 2004  133.85833  113.49100  15.2%     -  102s\n",
            "\n",
            "Cutting planes:\n",
            "  Learned: 1\n",
            "  Lift-and-project: 2\n",
            "  Cover: 1\n",
            "  Implied bound: 2400\n",
            "  Clique: 793\n",
            "  MIR: 428\n",
            "  Flow cover: 1484\n",
            "  RLT: 36\n",
            "  Relax-and-lift: 54\n",
            "  BQP: 2\n",
            "  PSD: 79\n",
            "\n",
            "Explored 1 nodes (241007 simplex iterations) in 102.36 seconds (170.35 work units)\n",
            "Thread count was 16 (of 16 available processors)\n",
            "\n",
            "Solution count 10: 133.858 134.274 134.285 ... 139.205\n",
            "\n",
            "Time limit reached\n",
            "Best objective 1.338583333430e+02, best bound 1.134910042343e+02, gap 15.2156%\n"
          ]
        }
      ],
      "source": [
        "# === Pick a query by index and run the workflow ===\n",
        "QUERY_INDEX = 7  # <-- change this to run a different query\n",
        "\n",
        "q = eval_queries[QUERY_INDEX]\n",
        "print(f\"[{q['id']}]  {q['category']} / {q['subcategory']}\")\n",
        "print(f\"Query:    {q['query']}\")\n",
        "print(f\"Expected: path={q['expected_path']}, expr={q['expected_constraint_expr']}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "final_state = invoke_workflow(\n",
        "    workflow,\n",
        "    q[\"query\"],\n",
        "    baseline_result=baseline_result,\n",
        ")\n",
        "\n",
        "actual_path = final_state.get(\"counterfactual_status\", \"unknown\")\n",
        "actual_answer = final_state.get(\"final_summary\", \"(no summary)\")\n",
        "actual_exprs = final_state.get(\"constraint_expressions\", [])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- ACTUAL ---\n",
            "Path:        feasible  MATCH\n",
            "Constraints: ['x_idle[5,0] == 0']\n",
            "Answer:                   The user's change resulted in a slight increase in the total objective by 4.05, or\n",
            "             about 3.1%. Notably, the **cost of missing demand** increased by 0.4500 due to\n",
            "             higher unmet staffing needs for several projects, which affected the demand\n",
            "             fulfillment. In contrast, the **staffing consistency** improved significantly by\n",
            "             3.0, indicating a more stable work assignment structure despite the rising idle\n",
            "             time, which also increased by 0.6000. This highlights a trade-off where improved\n",
            "             consistency in project assignments led to greater idle time for some resources\n",
            "             like Yimin during week 0, suggesting that while certain staff were better utilized\n",
            "             across other tasks, not all could be reassigned effectively, resulting in some\n",
            "             downtime.\n",
            "\n",
            "--- REFERENCE ---\n",
            "Path:        feasible\n",
            "Constraint:  x_idle[5,0] == 0\n",
            "Theme:       trade-off: assigning Yimin more would reduce idle but increase other costs\n",
            "Answer:                   Yimin (j=5, Lead, IO cohort, FTE=1.5) is 50% idle in week 0 (x_idle[5,0]=0.5). He\n",
            "             is partially allocated to IO product demo and Ipp IO Pilot. The optimizer doesn't\n",
            "             fully utilize Yimin in week 0 because: (1) IO Base development demand is 1.5 FTE\n",
            "             and has low cost (0.6), so the optimizer prefers to leave it understaffed rather\n",
            "             than use a high-FTE Lead; (2) Assigning Yimin to more projects would increase\n",
            "             staffing consistency cost (Obj 3) and potentially out-of-cohort penalty (Obj 4) if\n",
            "             assigned outside IO. The idle time cost (1 unit per FTE-week) is less than the\n",
            "             marginal cost of the fragmentation and cohort penalties that would result.\n"
          ]
        }
      ],
      "source": [
        "import textwrap\n",
        "\n",
        "print(f\"\\n--- ACTUAL ---\")\n",
        "print(f\"Path:        {actual_path}  {'MATCH' if actual_path == q['expected_path'] else 'MISMATCH'}\")\n",
        "print(f\"Constraints: {actual_exprs}\")\n",
        "wrapped_actual_answer = textwrap.fill(actual_answer, width=95, initial_indent=\"             \", subsequent_indent=\"             \") if actual_answer else actual_answer\n",
        "print(f\"Answer:      {wrapped_actual_answer}\")\n",
        "print(f\"\\n--- REFERENCE ---\")\n",
        "print(f\"Path:        {q['expected_path']}\")\n",
        "print(f\"Constraint:  {q['expected_constraint_expr']}\")\n",
        "print(f\"Theme:       {q['expected_answer_theme']}\")\n",
        "wrapped_reference_answer = textwrap.fill(q['reference_answer'], width=95, initial_indent=\"             \", subsequent_indent=\"             \") if q.get('reference_answer') else q['reference_answer']\n",
        "print(f\"Answer:      {wrapped_reference_answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Debug & Evaluation\n",
        "\n",
        "For **quick debug**: uncomment and run `from agentic_explain.workflow import debug; debug.print_workflow_summary(final_state)` in the cell above to print retrieval, LLM messages, applied constraints, and comparison in one go.\n",
        "\n",
        "For **comparing Plain RAG vs Graph RAG vs No-RAG**, use `notebooks/RAGComparison.ipynb`.\n",
        "\n",
        "### 5a. Retrieval Debug: Retrieved Chunks & Scores\n",
        "\n",
        "After running a query above, inspect which RAG chunks were retrieved and their relevance scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==========================================================================================\n",
            "  Stage: constraint_generation\n",
            "  Retrieval query: Force j=5 to not be idle in t=0.\n",
            "  Top-k: 5\n",
            "==========================================================================================\n",
            "\n",
            "  ── Chunk 0  score=0.7746\n",
            "     source=lp  section=constraints\n",
            "     {'constraint_name': 'staffed_indicator_0_5_5'}\n",
            "     Constraint: staffed_indicator_0_5_5 - x[5,0,5] - x[5,1,5] - x[5,2,5] - x[5,3,5]\n",
            "\n",
            "  ── Chunk 1  score=0.7740\n",
            "     source=lp  section=constraints\n",
            "     {'constraint_name': 'staffed_indicator_1_5_5'}\n",
            "     Constraint: staffed_indicator_1_5_5 - x[5,0,5] - x[5,1,5] - x[5,2,5] - x[5,3,5]\n",
            "\n",
            "  ── Chunk 2  score=0.7734\n",
            "     source=lp  section=constraints\n",
            "     {'constraint_name': 'staffed_indicator_0_5_1'}\n",
            "     Constraint: staffed_indicator_0_5_1 - x[5,0,1] - x[5,1,1] - x[5,2,1] - x[5,3,1]\n",
            "\n",
            "  ── Chunk 3  score=0.7732\n",
            "     source=lp  section=constraints\n",
            "     {'constraint_name': 'staffed_indicator_0_5_0'}\n",
            "     Constraint: staffed_indicator_0_5_0 - x[5,0,0] - x[5,1,0] - x[5,2,0] - x[5,3,0]\n",
            "\n",
            "  ── Chunk 4  score=0.7714\n",
            "     source=lp  section=constraints\n",
            "     {'constraint_name': 'staffed_indicator_1_5_1'}\n",
            "     Constraint: staffed_indicator_1_5_1 - x[5,0,1] - x[5,1,1] - x[5,2,1] - x[5,3,1]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import textwrap\n",
        "\n",
        "rag_debug = final_state.get(\"rag_retrieval_debug\", {})\n",
        "\n",
        "for stage_name, info in rag_debug.items():\n",
        "    print(f\"{'=' * 90}\")\n",
        "    print(f\"  Stage: {stage_name}\")\n",
        "    print(f\"  Retrieval query: {info.get('query', '?')}\")\n",
        "    print(f\"  Top-k: {info.get('top_k', '?')}\")\n",
        "    if \"iis_constraint_names\" in info:\n",
        "        print(f\"  IIS constraints: {info['iis_constraint_names']}\")\n",
        "    print(f\"{'=' * 90}\")\n",
        "\n",
        "    for i, chunk in enumerate(info.get(\"chunks\", [])):\n",
        "        score = chunk.get(\"score\")\n",
        "        meta = chunk.get(\"metadata\", {})\n",
        "        text = chunk.get(\"text\", \"\")\n",
        "        print(f\"\\n  ── Chunk {i}  score={score:.4f}\" if score is not None else f\"\\n  ── Chunk {i}  score=N/A\")\n",
        "        print(f\"     source={meta.get('source', '?')}  section={meta.get('section', '?')}\")\n",
        "        extra = {k: v for k, v in meta.items() if k not in (\"source\", \"section\", \"path\")}\n",
        "        if extra:\n",
        "            print(f\"     {extra}\")\n",
        "        # Show text (truncated & wrapped)\n",
        "        display = text if len(text) <= 500 else text[:500] + \"\\n     ... [truncated]\"\n",
        "        wrapped = textwrap.fill(display, width=95, initial_indent=\"     \", subsequent_indent=\"     \")\n",
        "        print(wrapped)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5b. LLM Messages Debug\n",
        "\n",
        "The exact system + user messages sent to the LLM at each RAG-augmented stage, and the raw response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==========================================================================================\n",
            "  Stage: constraint_generation\n",
            "==========================================================================================\n",
            "\n",
            "  [SYSTEM MESSAGE]\n",
            "  You translate a user request into one or more constraint expressions for a Gurobi\n",
            "  optimization model. Available decision variables (use exact names): d_miss, x, x_idle, x_ind,\n",
            "  x_p_ind. Format: variable_name[index1,index2,...] == value (integer indices, no spaces in\n",
            "  brackets). Example format: d_miss[0,0] == 1, x[0,0] == 1, x_idle[0,0] == 1. Use the RAG\n",
            "  context below to understand variable dimensions and index meanings. Output only the\n",
            "  constraint line(s), one per line, no explanation.\n",
            "\n",
            "  [USER MESSAGE]  (first 3000 chars)\n",
            "  RAG context: Constraint: staffed_indicator_0_5_5 - x[5,0,5] - x[5,1,5] - x[5,2,5] - x[5,3,5]\n",
            "  Constraint: staffed_indicator_1_5_5 - x[5,0,5] - x[5,1,5] - x[5,2,5] - x[5,3,5] Constraint:\n",
            "  staffed_indicator_0_5_1 - x[5,0,1] - x[5,1,1] - x[5,2,1] - x[5,3,1] Constraint:\n",
            "  staffed_indicator_0_5_0 - x[5,0,0] - x[5,1,0] - x[5,2,0] - x[5,3,0] Constraint:\n",
            "  staffed_indicator_1_5_1 - x[5,0,1] - x[5,1,1] - x[5,2,1] - x[5,3,1]  User request: Force j=5\n",
            "  to not be idle in t=0.\n",
            "\n",
            "  [RAW LLM RESPONSE]\n",
            "  x_idle[5,0] == 0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "CHARS_TO_SHOW = 3000\n",
        "llm_debug = final_state.get(\"llm_messages_debug\", {})\n",
        "\n",
        "for stage_name, info in llm_debug.items():\n",
        "    print(f\"{'=' * 90}\")\n",
        "    print(f\"  Stage: {stage_name}\")\n",
        "    print(f\"{'=' * 90}\")\n",
        "\n",
        "    print(f\"\\n  [SYSTEM MESSAGE]\")\n",
        "    sys_msg = info.get(\"system\", \"\")\n",
        "    print(textwrap.fill(sys_msg, width=95, initial_indent=\"  \", subsequent_indent=\"  \"))\n",
        "\n",
        "    print(f\"\\n  [USER MESSAGE]  (first {CHARS_TO_SHOW} chars)\")\n",
        "    user_msg = info.get(\"user\", \"\")\n",
        "    display_user = user_msg if len(user_msg) <= CHARS_TO_SHOW else user_msg[:CHARS_TO_SHOW] + \"\\n  ... [truncated]\"\n",
        "    print(textwrap.fill(display_user, width=95, initial_indent=\"  \", subsequent_indent=\"  \"))\n",
        "\n",
        "    print(f\"\\n  [RAW LLM RESPONSE]\")\n",
        "    raw = info.get(\"raw_response\", \"\")\n",
        "    print(textwrap.fill(raw, width=95, initial_indent=\"  \", subsequent_indent=\"  \"))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5c. Applied Constraints Debug\n",
        "\n",
        "Which constraints were actually added to the Gurobi model, and what were the baseline values of those variables?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==========================================================================================\n",
            "  1 constraint(s) added to the counterfactual Gurobi model\n",
            "==========================================================================================\n",
            "\n",
            "  Constraint 0:\n",
            "    Expression:      x_idle[5,0] == 0\n",
            "    Gurobi var:      x_idle[5,0]\n",
            "    Forced value:    0.0  (forcing DOWN from 0.5000)\n",
            "    Baseline value:  0.5000\n",
            "    Var type:        C  bounds=[0.0, 1.0]\n",
            "    Constr name:     user_constr_x_idle_5_0\n",
            "\n",
            "  Counterfactual status: feasible\n",
            "  Baseline obj:          129.8083\n",
            "  Counterfactual obj:    133.8583\n",
            "  Delta:                 +4.0500\n"
          ]
        }
      ],
      "source": [
        "cf_result = final_state.get(\"counterfactual_result\", {})\n",
        "applied = cf_result.get(\"applied_constraints\", [])\n",
        "\n",
        "if not applied:\n",
        "    print(\"No constraints were applied (check counterfactual_result for errors).\")\n",
        "    if cf_result.get(\"error\"):\n",
        "        print(f\"  Error: {cf_result['error']}\")\n",
        "else:\n",
        "    print(f\"{'=' * 90}\")\n",
        "    print(f\"  {len(applied)} constraint(s) added to the counterfactual Gurobi model\")\n",
        "    print(f\"{'=' * 90}\")\n",
        "    for i, ac in enumerate(applied):\n",
        "        baseline_val = ac.get(\"baseline_value\")\n",
        "        forced_val = ac.get(\"forced_value\")\n",
        "        bv_str = f\"{baseline_val:.4f}\" if baseline_val is not None else \"N/A\"\n",
        "        direction = \"\"\n",
        "        if baseline_val is not None:\n",
        "            if abs(forced_val - baseline_val) < 1e-8:\n",
        "                direction = \"(no change from baseline)\"\n",
        "            elif forced_val > baseline_val:\n",
        "                direction = f\"(forcing UP from {bv_str})\"\n",
        "            else:\n",
        "                direction = f\"(forcing DOWN from {bv_str})\"\n",
        "\n",
        "        print(f\"\\n  Constraint {i}:\")\n",
        "        print(f\"    Expression:      {ac.get('expr')}\")\n",
        "        print(f\"    Gurobi var:      {ac.get('gurobi_var_name')}\")\n",
        "        print(f\"    Forced value:    {forced_val}  {direction}\")\n",
        "        print(f\"    Baseline value:  {bv_str}\")\n",
        "        print(f\"    Var type:        {ac.get('var_type')}  bounds=[{ac.get('var_lb')}, {ac.get('var_ub')}]\")\n",
        "        print(f\"    Constr name:     {ac.get('constraint_name')}\")\n",
        "\n",
        "    print(f\"\\n  Counterfactual status: {final_state.get('counterfactual_status')}\")\n",
        "    cf_obj = cf_result.get(\"objective_value\")\n",
        "    base_obj = final_state.get(\"baseline_result\", {}).get(\"objective_value\")\n",
        "    if cf_obj is not None and base_obj is not None:\n",
        "        print(f\"  Baseline obj:          {base_obj:.4f}\")\n",
        "        print(f\"  Counterfactual obj:    {cf_obj:.4f}\")\n",
        "        print(f\"  Delta:                 {cf_obj - base_obj:+.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5d. Detailed Objective Comparison & Variable Changes\n",
        "\n",
        "The compare node produces a structured breakdown of all four objective terms (baseline vs counterfactual)\n",
        "and highlights which variables changed to cause the differences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== OBJECTIVE COMPARISON ===\n",
            "Term                                    Baseline     Counter.        Delta    %Change\n",
            "─────────────────────────────────────────────────────────────────────────────────────\n",
            "cost_of_missing_demand                   56.0750      56.5250      +0.4500      +0.8%\n",
            "  (Weighted sum of unmet staffing demand across all projects and weeks)\n",
            "idle_time                                28.7333      29.3333      +0.6000      +2.1%\n",
            "  (Total employee idle time (FTE-weeks not assigned to any project))\n",
            "staffing_consistency                     43.0000      46.0000      +3.0000      +7.0%\n",
            "  (Number of unique employee-project pairings (fewer = less context switching))\n",
            "out_of_cohort_penalty                     2.0000       2.0000      +0.0000      +0.0%\n",
            "  (Penalty for assigning employees to projects outside their preferred cohort)\n",
            "─────────────────────────────────────────────────────────────────────────────────────\n",
            "TOTAL                                   129.8083     133.8583      +4.0500      +3.1%\n",
            "\n",
            "=== VARIABLE CHANGES BY FAMILY ===\n",
            "\n",
            "d_miss — 48 variables changed (top 15 by magnitude):\n",
            "  ▲ d_miss[7,8]: 0.2500 → 1.5000 (+1.2500)\n",
            "  ▼ d_miss[11,5]: 1.0000 → 0.0000 (-1.0000)\n",
            "  ▲ d_miss[11,14]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▼ d_miss[13,5]: 1.0000 → 0.0000 (-1.0000)\n",
            "  ▲ d_miss[21,14]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▼ d_miss[21,21]: 1.0000 → 0.0000 (-1.0000)\n",
            "  ▲ d_miss[9,14]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▼ d_miss[7,14]: 0.7500 → 0.0000 (-0.7500)\n",
            "  ▲ d_miss[13,3]: -0.0000 → 0.5000 (+0.5000)\n",
            "  ▼ d_miss[6,0]: 0.5000 → 0.0000 (-0.5000)\n",
            "  ▼ d_miss[7,0]: 0.5000 → 0.0000 (-0.5000)\n",
            "  ▲ d_miss[8,3]: 0.0000 → 0.5000 (+0.5000)\n",
            "  ▲ d_miss[8,8]: 1.0000 → 1.5000 (+0.5000)\n",
            "  ▼ d_miss[9,18]: 0.5000 → 0.0000 (-0.5000)\n",
            "  ▼ d_miss[11,18]: 0.5000 → 0.0000 (-0.5000)\n",
            "  ... and 33 more\n",
            "\n",
            "x — 580 variables changed (top 15 by magnitude):\n",
            "  ▲ x[0,13,6]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▲ x[0,17,6]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▲ x[0,18,6]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▲ x[1,0,4]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▼ x[1,0,6]: 1.0000 → 0.0000 (-1.0000)\n",
            "  ▲ x[1,1,4]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▼ x[1,1,6]: 1.0000 → 0.0000 (-1.0000)\n",
            "  ▲ x[1,2,4]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▼ x[1,2,6]: 1.0000 → 0.0000 (-1.0000)\n",
            "  ▲ x[1,3,4]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▼ x[1,3,6]: 1.0000 → 0.0000 (-1.0000)\n",
            "  ▲ x[1,4,4]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▼ x[1,4,6]: 1.0000 → 0.0000 (-1.0000)\n",
            "  ▲ x[1,5,4]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▼ x[1,5,6]: 1.0000 → 0.0000 (-1.0000)\n",
            "  ... and 565 more\n",
            "\n",
            "x_idle — 19 variables changed (top 15 by magnitude):\n",
            "  ▼ x_idle[1,24]: 1.0000 → 0.0000 (-1.0000)\n",
            "  ▼ x_idle[1,25]: 1.0000 → 0.0000 (-1.0000)\n",
            "  ▲ x_idle[2,24]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▲ x_idle[2,25]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▼ x_idle[5,0]: 0.5000 → 0.0000 (-0.5000)\n",
            "  ▲ x_idle[0,0]: 0.0000 → 0.3333 (+0.3333)\n",
            "  ▲ x_idle[5,8]: 0.0000 → 0.3333 (+0.3333)\n",
            "  ▲ x_idle[13,9]: 0.0000 → 0.2000 (+0.2000)\n",
            "  ▲ x_idle[10,1]: 0.0000 → 0.2000 (+0.2000)\n",
            "  ▲ x_idle[4,3]: 0.0000 → 0.2000 (+0.2000)\n",
            "  ▲ x_idle[4,4]: 0.0000 → 0.2000 (+0.2000)\n",
            "  ▲ x_idle[10,0]: 0.0000 → 0.2000 (+0.2000)\n",
            "  ▲ x_idle[10,3]: 0.0000 → 0.2000 (+0.2000)\n",
            "  ▼ x_idle[13,4]: 0.2000 → 0.0000 (-0.2000)\n",
            "  ▼ x_idle[13,6]: 0.2000 → 0.0000 (-0.2000)\n",
            "  ... and 4 more\n",
            "\n",
            "x_ind — 443 variables changed (top 15 by magnitude):\n",
            "  ▼ x_ind[0,0,2]: 1.0000 → -0.0000 (-1.0000)\n",
            "  ▼ x_ind[0,1,1]: 1.0000 → -0.0000 (-1.0000)\n",
            "  ▼ x_ind[0,2,1]: 1.0000 → -0.0000 (-1.0000)\n",
            "  ▼ x_ind[0,4,1]: 1.0000 → -0.0000 (-1.0000)\n",
            "  ▼ x_ind[0,6,1]: 1.0000 → -0.0000 (-1.0000)\n",
            "  ▼ x_ind[0,7,1]: 1.0000 → -0.0000 (-1.0000)\n",
            "  ▲ x_ind[0,7,6]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▼ x_ind[0,8,1]: 1.0000 → -0.0000 (-1.0000)\n",
            "  ▼ x_ind[0,9,1]: 1.0000 → -0.0000 (-1.0000)\n",
            "  ▲ x_ind[0,9,6]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▲ x_ind[0,11,6]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▲ x_ind[0,13,6]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▼ x_ind[0,14,1]: 1.0000 → -0.0000 (-1.0000)\n",
            "  ▲ x_ind[0,14,6]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▼ x_ind[0,15,1]: 1.0000 → -0.0000 (-1.0000)\n",
            "  ... and 428 more\n",
            "\n",
            "x_p_ind — 29 variables changed (top 15 by magnitude):\n",
            "  ▼ x_p_ind[0,1]: 1.0000 → -0.0000 (-1.0000)\n",
            "  ▲ x_p_ind[0,6]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▲ x_p_ind[1,4]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▼ x_p_ind[1,6]: 1.0000 → -0.0000 (-1.0000)\n",
            "  ▲ x_p_ind[2,1]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▼ x_p_ind[2,4]: 1.0000 → -0.0000 (-1.0000)\n",
            "  ▼ x_p_ind[3,1]: 1.0000 → -0.0000 (-1.0000)\n",
            "  ▲ x_p_ind[3,6]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▼ x_p_ind[4,3]: 1.0000 → -0.0000 (-1.0000)\n",
            "  ▼ x_p_ind[5,19]: 1.0000 → -0.0000 (-1.0000)\n",
            "  ▲ x_p_ind[7,9]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▲ x_p_ind[8,6]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▲ x_p_ind[8,8]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▲ x_p_ind[9,6]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ▲ x_p_ind[9,14]: 0.0000 → 1.0000 (+1.0000)\n",
            "  ... and 14 more\n"
          ]
        }
      ],
      "source": [
        "# Print the full comparison summary (generated by the compare node)\n",
        "comparison = final_state.get(\"comparison_summary\", \"(no comparison)\")\n",
        "print(comparison)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === (Optional) Batch run: evaluate all queries and collect results ===\n",
        "# Uncomment and run to evaluate the full dataset.\n",
        "# Results are saved to outputs/eval_results.json for later analysis.\n",
        "\n",
        "# eval_results = []\n",
        "# for i, q in enumerate(eval_queries):\n",
        "#     print(f\"\\n[{i}/{len(eval_queries)}] {q['id']}: {q['query'][:60]}...\")\n",
        "#     state = invoke_workflow(\n",
        "#         workflow, q[\"query\"],\n",
        "#         baseline_result=baseline_result,\n",
        "#     )\n",
        "#     eval_results.append({\n",
        "#         \"query_id\": q[\"id\"],\n",
        "#         \"query\": q[\"query\"],\n",
        "#         \"expected_path\": q[\"expected_path\"],\n",
        "#         \"actual_path\": state.get(\"counterfactual_status\", \"unknown\"),\n",
        "#         \"path_match\": state.get(\"counterfactual_status\") == q[\"expected_path\"],\n",
        "#         \"expected_constraint_expr\": q[\"expected_constraint_expr\"],\n",
        "#         \"actual_constraint_exprs\": state.get(\"constraint_expressions\", []),\n",
        "#         \"actual_answer\": state.get(\"final_summary\", \"\"),\n",
        "#         \"reference_answer\": q[\"reference_answer\"],\n",
        "#         \"expected_answer_theme\": q[\"expected_answer_theme\"],\n",
        "#     })\n",
        "#     print(f\"  Path: {state.get('counterfactual_status')} \"\n",
        "#           f\"{'MATCH' if state.get('counterfactual_status') == q['expected_path'] else 'MISMATCH'}\")\n",
        "#\n",
        "# with open(OUTPUTS_DIR / \"eval_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "#     json.dump(eval_results, f, indent=2)\n",
        "# \n",
        "# n_match = sum(1 for r in eval_results if r[\"path_match\"])\n",
        "# print(f\"\\n=== Summary ===\")\n",
        "# print(f\"Path match: {n_match}/{len(eval_results)} ({100*n_match/len(eval_results):.0f}%)\")\n",
        "# print(f\"Results saved to {OUTPUTS_DIR / 'eval_results.json'}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rag",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
